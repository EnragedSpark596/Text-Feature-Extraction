{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# http://www.sefidian.com/2022/02/28/understanding-tf-idf-with-python-example/\n",
    "# https://www.techiedelight.com/initialize-list-of-lists-python/\n",
    "# https://pynative.com/python-file-open/\n",
    "# http://www.pythonforbeginners.com/cheatsheet/python-file-handling\n",
    "# https://www.geeksforgeeks.org/python-spilt-a-sentence-into-list-of-words/\n",
    "# https://www.w3schools.com/python/python_ref_string.asp\n",
    "# https://en.wikipedia.org/wiki/Tf%E2%80%93idf\n",
    "# Python/Udemy_NLP_JPortilla/UPDATED_NLP_COURSE/03-Text-Classification/01-Feature-Extraction-from-Text.ipynb\n",
    "# https://stackoverflow.com/questions/32768555/find-the-set-of-column-indices-for-non-zero-values-in-each-row-in-pandas-data-f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Objective: find commonest words that are unique to single doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error handling needs:\n",
    "# #if len(docNames) =/= len(filenames)\n",
    "# empty lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.0 Imports\n",
    "# 2.0 Functions\n",
    "    # getText\n",
    "# 3.0 Global variables\n",
    "    # filepath\n",
    "    # filenames\n",
    "    # docName\n",
    "    # stem.verbs = [verb_stems.txt1, verb_stems.txt2, etc.]    - document-specific unique list of text strings\n",
    "    # nouns = [doc.1_nouns, doc.2_nouns, etc.]                 - list of nouns unique to each doc\n",
    "    # adj = [doc1.adj, etc]\n",
    "    # adv = [doc1.adv, etc]\n",
    "    \n",
    "# 4.0 set-up - populate docNames with text file contents\n",
    "# 5.0 Clean docNames[files] ready for spacy, TF-IDF: -each txt file should have a vocab list of stem_verbs, nouns, adj, adv\n",
    "    # make content lower case\n",
    "    # remove stopwords and dropwords\n",
    "    # remove punctuation\n",
    "# 6.0 prep for POS extraction\n",
    "    # spacy\n",
    "#7.0 Gather bulk data\n",
    "    # make a corpus from all 8 files\n",
    "#8.0 All Text\n",
    "    # make a word-set (vocab of unique words) of the corpus\n",
    "    # manual Term Frequeny calc\n",
    "    # manual inverse document frequency (IDF) calc\n",
    "    # apply sklearn TfidfVectorizer\n",
    "# 9.0 Nouns\n",
    "    # make a corpus from all 8 files\n",
    "    # make a word-set (vocab of unique words) of the corpus\n",
    "    # manual Term Frequeny calc\n",
    "    # manual inverse document frequency (IDF) calc\n",
    "    # apply sklearn TfidfVectorizer\n",
    "# 10.0 Verbs\n",
    "    # make a corpus from all 8 files\n",
    "    # make a word-set (vocab of unique words) of the corpus\n",
    "    # manual Term Frequeny calc\n",
    "    # manual inverse document frequency (IDF) calc\n",
    "    # apply sklearn TfidfVectorizer\n",
    "# 11.0 Adjs and Advs\n",
    "    # make a corpus from all 8 files\n",
    "    # make a word-set (vocab of unique words) of the corpus\n",
    "    # manual Term Frequeny calc\n",
    "    # manual inverse document frequency (IDF) calc\n",
    "    # apply sklearn TfidfVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ianda\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# 1.0 Imports\n",
    "import string\n",
    "import nltk\n",
    "import re\n",
    "import nltk.corpus\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.0 Functions\n",
    "    # getText\n",
    "def getText(txtFileName):\n",
    "    with open(txtFileName,'r') as txtfile:\n",
    "        reader = txtfile.read()\n",
    "    return reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.0 Global variables\n",
    "    # filepath\n",
    "filepath = 'C://Users//ianda//Dropbox (NSF CORE)//Coding Projects//Python//NSN_RevHerPodcast_TF-IDF//Test_Text_Files//'\n",
    "\n",
    "    # filenames\n",
    "filenames = ['Metamorphosis1.txt','Metamorphosis2b.txt','Metamorphosis3.txt','Metamorphosis4.txt','Metamorphosis5.txt',\n",
    "             'Metamorphosis6.txt','Metamorphosis7.txt','Metamorphosis8.txt']\n",
    "\n",
    "# set size of empty lists to number of filenames\n",
    "list_len = len(filenames)\n",
    "\n",
    "    # docName container for collection of lists of strings (strings = body copy of transcript)\n",
    "#docs to be empty lists which will contain 1 string (the whole of one text file) - OG text\n",
    "\n",
    "docNames = [[] for x in range(list_len)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.0 set-up - populate docNames with text file contents\n",
    "\n",
    "# FIX THIS\n",
    "#if len(docNames) == len(filenames):\n",
    "    \n",
    "#    Else\n",
    "#End\n",
    "\n",
    "for i in range(len(filenames)):\n",
    "    fileName = filepath + filenames[i]\n",
    "    docNames[i] = getText(fileName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(docNames[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.0 Clean docNames[files] ready for spacy, TF-IDF: -each txt file should have a vocab list of stem_verbs, nouns, adj, adv\n",
    "    # make entire content lower case\n",
    "all_lower = [[] for x in range(list_len)]\n",
    "\n",
    "for i in range(len(filenames)):\n",
    "    all_lower[i] = docNames[i].lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_lower[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # 5.1 remove stopwords and dropwords DO BEFORE DEPUNC\n",
    "stop = stopwords.words('english')\n",
    "dropWords = ['oh','wow', 'so','goodness', 'yeah']\n",
    "\n",
    "all_stopped = [[] for x in range(list_len)]\n",
    "\n",
    "for i in range(len(filenames)):\n",
    "        all_stopped[i] = \" \".join([word for word in all_lower[i].split() if word not in (stop) and word not in (dropWords)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_stopped[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # 5.2 remove punctuation\n",
    "# https://monkeylearn.com/blog/text-cleaning/\n",
    "# punctuation removal attempt - DO AFTER DROP/STOP\n",
    "all_depunc = [[] for x in range(list_len)]\n",
    "\n",
    "for i in range(len(filenames)):\n",
    "        all_depunc[i] = re.sub(r\"(@\\[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)|^rt|http.+?\", \"\", all_stopped[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_depunc[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.0 prep for POS extraction\n",
    "    # spacy\n",
    "# make list of spacy tokens for each file\n",
    "all_spacy_nlp = [[] for x in range(list_len)]\n",
    "\n",
    "for i in range(len(filenames)):\n",
    "    all_spacy_nlp[i] = nlp(all_depunc[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_spacy_nlp[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#7.0 Gather and sort the data\n",
    "    # Create full text then separate out each POS\n",
    "\n",
    "# verbs will need to be stemmed for proper comparison which means they will need an additional processing step that\n",
    "# other POSs don't need\n",
    "\n",
    "# Select all token.pos as verb for doc_verb_stem.list\n",
    "all_verbs = [[] for x in range(list_len)]\n",
    "all_nouns = [[] for x in range(list_len)]\n",
    "all_adjs = [[] for x in range(list_len)]\n",
    "all_advs = [[] for x in range(list_len)]\n",
    "\n",
    "# need to figure out why this needs a double indent to work\n",
    "for i in range(len(filenames)):\n",
    "        all_verbs[i] = [token for token in all_spacy_nlp[i] if token.pos_ == 'VERB']\n",
    "        all_nouns[i] = [str(token) for token in all_spacy_nlp[i] if token.pos_ == 'NOUN']\n",
    "        all_adjs[i] = [str(token) for token in all_spacy_nlp[i] if token.pos_ == 'ADJ']\n",
    "        all_advs[i] = [str(token) for token in all_spacy_nlp[i] if token.pos_ == 'ADV']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.0 Gather bulk data\n",
    "corpus_all = [string for string in all_depunc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(corpus_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(corpus_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(all_depunc[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(corpus_all[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.0 All text\n",
    "    # make a word-set (vocab of unique words) of the corpus\n",
    "    # manual Term Frequeny calc\n",
    "    # manual inverse document frequency (IDF) calc\n",
    "    # apply sklearn TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.1 create a word set/vocab for the corpus\n",
    "all_words_set = set()\n",
    " \n",
    "for doc in corpus_all:\n",
    "    words_all = doc.split(' ')\n",
    "    all_words_set = all_words_set.union(set(words_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in the corpus: 304\n"
     ]
    }
   ],
   "source": [
    "print('Number of words in the corpus:',len(all_words_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The words in the corpus: \n",
      " {'side', 'work', 'incredible', 'behavior', 'detract', 'recognize', 'hard', 'with', 'traveled', 'metamorphosis', 'belong', 'dictation', 'imagine', 'saying', 'around', 'pushing', 'scenery', 'family', 'place', 'trick', 'further', 'vast', 'beautiful', 'you', 'up', 'thoughts', 'yeah', 'lived', 'outside', 'no', 'be', 'year', 'studies', 'still', 'today', 'versions', 'instructed', 'trusted', 'kind', 'challenging', 'several', 'surround', 'need', 'them', 'journey', 'journaling', 'multidimensional', 'memos', 'background', 'get', 'career', 'friends', 'slightly', 'want', 'simple', 'transformation', 'old', 'ways', 'authentic', 'check', 'tips', 'therapeutic', 'like', 'heart', 'placebo', 'from', 'say', 'write', 'attracted', 'husband', 'two', 'even', 'happen', 'ill', 'another', 'without', 'stream', 'weeks', 'sort', 'comfort', 'one', 'exceeding', 'three', 'fact', 'have', 'again', 'mentally', 'dimensions', 'unquote', 'study', 'feel', 'myself', 'self', 'tools', 'topic', 'however', 'went', 'overnight', 'wrong', 'motivation', 'number', 'kids', 'stretching', 'patience', 'whatever', 'detractors', 'express', 'fundamental', 'scared', 'goals', 'alone', 'think', 'boundaries', 'privately', 'position', 'this', 'startup', 'so', 'forest', 'nervous', 'may', '20s', 'regrets', 'shame', 'early', 'loved', 'call', 'cant', 'approach', 'versus', 'draw', 'spoke', 'rates', 'vulnerable', 'yourself', 'wellness', 'there', 'important', 'real', 'way', 'spouse', 'supporters', 'felt', 'sales', 'needed', 'could', 'growth', 'school', 'smaller', 'great', 'evolved', 'grow', 'theyre', 'partner', 'best', 'untouched', 'technology', 'despite', 'try', 'im', 'practice', 'much', 'corny', 'years', 'zone', 'markers', 'beliefs', 'working', 'things', 'steps', 'companies', 'people', 'ive', 'fortunate', 'house', 'barriers', 'whats', 'water', 'wake', 'knowingly', 'quickly', 'crystallize', 'along', 'areas', 'minute', 'yet', 'emerging', 'whether', 'make', 'broken', 'blood', '80', 'took', 'makes', 'quote', 'clearly', 'catch', 'resting', 'parenting', 'something', 'absolutely', 'somewhat', 'clinical', 'including', 'achieve', 'look', 'nearby', 'direction', 'pressure', 'admit', 'right', 'capture', 'confidence', 'essentially', 'anything', 'potential', 'own', 'told', 'effectively', 'men', 'past', 'setting', 'imagining', 'describe', 'ones', 'would', 'journal', 'nature', 'challenge', 'finished', 'voice', 'happy', 'course', 'been', 'hold', 'experts', 'shift', 'mental', 'sad', 'take', 'moment', 'really', 'future', 'struggling', 'that', 'grounding', 'thats', 'listen', 'low', 'tell', 'see', 'kinds', 'changes', 'key', 'seeing', 'mirror', 'although', 'paper', 'pitch', 'thing', 'brain', 'present', 'disciplines', 'reflect', 'takes', 'change', 'children', 'home', 'sense', 'keep', 'globe', 'talk', 'overwhelmed', 'pharmaceutical', 'octogenarian', 'definitely', 'child', 'form', 'keeps', 'know', 'biological', 'shifted', 'based', 'guess', 'days', 'quiet', 'beginning', 'employed', 'within', 'little', 'well', 'almost', 'different', 'app', 'needs', 'small', 'time', 'were', 'strategies', 'part', 'many', 'consciousness', 'evermore', 'pretty'}\n"
     ]
    }
   ],
   "source": [
    "print('The words in the corpus: \\n', all_words_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.2 manual calculation of Term Frequency\n",
    "\n",
    "n_docs_all = len(corpus_all)         #·Number of documents in the corpus\n",
    "n_words_set_all = len(all_words_set) #·Number of unique words in the\n",
    " \n",
    "df_tf_all = pd.DataFrame(np.zeros((n_docs_all, n_words_set_all)), columns=all_words_set)\n",
    " \n",
    "# Compute Term Frequency (TF)\n",
    "for i in range(n_docs_all):\n",
    "    words = corpus_all[i].split(' ') # Words in the document\n",
    "    for w in words:\n",
    "        df_tf_all[w][i] = df_tf_all[w][i] + (1 / len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>side</th>\n",
       "      <th>work</th>\n",
       "      <th>incredible</th>\n",
       "      <th>behavior</th>\n",
       "      <th>detract</th>\n",
       "      <th>recognize</th>\n",
       "      <th>hard</th>\n",
       "      <th>with</th>\n",
       "      <th>traveled</th>\n",
       "      <th>metamorphosis</th>\n",
       "      <th>...</th>\n",
       "      <th>needs</th>\n",
       "      <th>small</th>\n",
       "      <th>time</th>\n",
       "      <th>were</th>\n",
       "      <th>strategies</th>\n",
       "      <th>part</th>\n",
       "      <th>many</th>\n",
       "      <th>consciousness</th>\n",
       "      <th>evermore</th>\n",
       "      <th>pretty</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.029851</td>\n",
       "      <td>0.014925</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.014925</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.014925</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.014925</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.047619</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.025316</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012658</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012658</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012658</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012658</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.025316</td>\n",
       "      <td>0.012658</td>\n",
       "      <td>0.012658</td>\n",
       "      <td>0.012658</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012658</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012658</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.021505</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010753</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010753</td>\n",
       "      <td>0.010753</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012658</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.025316</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.026316</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.026316</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.026316</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 304 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       side      work  incredible  behavior   detract  recognize      hard  \\\n",
       "0  0.029851  0.014925    0.000000  0.000000  0.000000   0.000000  0.014925   \n",
       "1  0.000000  0.000000    0.000000  0.000000  0.000000   0.000000  0.000000   \n",
       "2  0.000000  0.000000    0.000000  0.000000  0.000000   0.000000  0.000000   \n",
       "3  0.000000  0.000000    0.025316  0.000000  0.000000   0.000000  0.000000   \n",
       "4  0.000000  0.000000    0.025316  0.012658  0.012658   0.012658  0.000000   \n",
       "5  0.000000  0.021505    0.000000  0.000000  0.000000   0.000000  0.000000   \n",
       "6  0.000000  0.012658    0.000000  0.000000  0.000000   0.000000  0.000000   \n",
       "7  0.000000  0.000000    0.000000  0.000000  0.000000   0.000000  0.000000   \n",
       "\n",
       "       with  traveled  metamorphosis  ...     needs     small      time  \\\n",
       "0  0.000000  0.000000       0.014925  ...  0.000000  0.000000  0.014925   \n",
       "1  0.000000  0.000000       0.000000  ...  0.000000  0.000000  0.000000   \n",
       "2  0.000000  0.000000       0.000000  ...  0.000000  0.000000  0.000000   \n",
       "3  0.012658  0.000000       0.000000  ...  0.000000  0.012658  0.000000   \n",
       "4  0.012658  0.000000       0.000000  ...  0.000000  0.012658  0.000000   \n",
       "5  0.000000  0.000000       0.000000  ...  0.000000  0.000000  0.000000   \n",
       "6  0.000000  0.000000       0.000000  ...  0.000000  0.025316  0.000000   \n",
       "7  0.000000  0.026316       0.000000  ...  0.026316  0.000000  0.000000   \n",
       "\n",
       "       were  strategies      part      many  consciousness  evermore    pretty  \n",
       "0  0.000000    0.000000  0.000000  0.000000       0.000000  0.000000  0.000000  \n",
       "1  0.000000    0.047619  0.000000  0.000000       0.000000  0.000000  0.000000  \n",
       "2  0.000000    0.000000  0.000000  0.000000       0.000000  0.000000  0.000000  \n",
       "3  0.012658    0.000000  0.000000  0.012658       0.000000  0.000000  0.000000  \n",
       "4  0.000000    0.000000  0.000000  0.000000       0.000000  0.000000  0.000000  \n",
       "5  0.000000    0.000000  0.010753  0.000000       0.010753  0.010753  0.000000  \n",
       "6  0.000000    0.000000  0.000000  0.000000       0.000000  0.000000  0.000000  \n",
       "7  0.000000    0.000000  0.000000  0.000000       0.000000  0.000000  0.026316  \n",
       "\n",
       "[8 rows x 304 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#This shows the frequency of each word in each document.\n",
    "df_tf_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IDF of: \n",
      "           side: 0.9030899869919435\n",
      "           work: 0.4259687322722811\n",
      "     incredible: 0.6020599913279624\n",
      "       behavior: 0.9030899869919435\n",
      "        detract: 0.9030899869919435\n",
      "      recognize: 0.9030899869919435\n",
      "           hard: 0.9030899869919435\n",
      "           with: 0.6020599913279624\n",
      "       traveled: 0.9030899869919435\n",
      "  metamorphosis: 0.9030899869919435\n",
      "         belong: 0.9030899869919435\n",
      "      dictation: 0.9030899869919435\n",
      "        imagine: 0.9030899869919435\n",
      "         saying: 0.9030899869919435\n",
      "         around: 0.9030899869919435\n",
      "        pushing: 0.9030899869919435\n",
      "        scenery: 0.9030899869919435\n",
      "         family: 0.6020599913279624\n",
      "          place: 0.9030899869919435\n",
      "          trick: 0.9030899869919435\n",
      "        further: 0.9030899869919435\n",
      "           vast: 0.9030899869919435\n",
      "      beautiful: 0.9030899869919435\n",
      "            you: 0.6020599913279624\n",
      "             up: 0.9030899869919435\n",
      "       thoughts: 0.9030899869919435\n",
      "           yeah: 0.6020599913279624\n",
      "          lived: 0.9030899869919435\n",
      "        outside: 0.9030899869919435\n",
      "             no: 0.9030899869919435\n",
      "             be: 0.9030899869919435\n",
      "           year: 0.9030899869919435\n",
      "        studies: 0.9030899869919435\n",
      "          still: 0.2041199826559248\n",
      "          today: 0.9030899869919435\n",
      "       versions: 0.9030899869919435\n",
      "     instructed: 0.9030899869919435\n",
      "        trusted: 0.6020599913279624\n",
      "           kind: 0.4259687322722811\n",
      "    challenging: 0.9030899869919435\n",
      "        several: 0.9030899869919435\n",
      "       surround: 0.9030899869919435\n",
      "           need: 0.6020599913279624\n",
      "           them: 0.9030899869919435\n",
      "        journey: 0.6020599913279624\n",
      "     journaling: 0.9030899869919435\n",
      "multidimensional: 0.9030899869919435\n",
      "          memos: 0.9030899869919435\n",
      "     background: 0.9030899869919435\n",
      "            get: 0.6020599913279624\n",
      "         career: 0.9030899869919435\n",
      "        friends: 0.4259687322722811\n",
      "       slightly: 0.9030899869919435\n",
      "           want: 0.6020599913279624\n",
      "         simple: 0.6020599913279624\n",
      " transformation: 0.9030899869919435\n",
      "            old: 0.6020599913279624\n",
      "           ways: 0.6020599913279624\n",
      "      authentic: 0.6020599913279624\n",
      "          check: 0.6020599913279624\n",
      "           tips: 0.6020599913279624\n",
      "    therapeutic: 0.9030899869919435\n",
      "           like: 0.2041199826559248\n",
      "          heart: 0.9030899869919435\n",
      "        placebo: 0.9030899869919435\n",
      "           from: 0.9030899869919435\n",
      "            say: 0.6020599913279624\n",
      "          write: 0.9030899869919435\n",
      "      attracted: 0.9030899869919435\n",
      "        husband: 0.9030899869919435\n",
      "            two: 0.9030899869919435\n",
      "           even: 0.9030899869919435\n",
      "         happen: 0.9030899869919435\n",
      "            ill: 0.9030899869919435\n",
      "        another: 0.6020599913279624\n",
      "        without: 0.9030899869919435\n",
      "         stream: 0.9030899869919435\n",
      "          weeks: 0.9030899869919435\n",
      "           sort: 0.9030899869919435\n",
      "        comfort: 0.6020599913279624\n",
      "            one: 0.6020599913279624\n",
      "      exceeding: 0.9030899869919435\n",
      "          three: 0.9030899869919435\n",
      "           fact: 0.9030899869919435\n",
      "           have: 0.9030899869919435\n",
      "          again: 0.9030899869919435\n",
      "       mentally: 0.9030899869919435\n",
      "     dimensions: 0.9030899869919435\n",
      "        unquote: 0.9030899869919435\n",
      "          study: 0.9030899869919435\n",
      "           feel: 0.4259687322722811\n",
      "         myself: 0.9030899869919435\n",
      "           self: 0.9030899869919435\n",
      "          tools: 0.9030899869919435\n",
      "          topic: 0.9030899869919435\n",
      "        however: 0.9030899869919435\n",
      "           went: 0.3010299956639812\n",
      "      overnight: 0.9030899869919435\n",
      "          wrong: 0.9030899869919435\n",
      "     motivation: 0.6020599913279624\n",
      "         number: 0.6020599913279624\n",
      "           kids: 0.9030899869919435\n",
      "     stretching: 0.9030899869919435\n",
      "       patience: 0.9030899869919435\n",
      "       whatever: 0.9030899869919435\n",
      "     detractors: 0.9030899869919435\n",
      "        express: 0.9030899869919435\n",
      "    fundamental: 0.9030899869919435\n",
      "         scared: 0.9030899869919435\n",
      "          goals: 0.6020599913279624\n",
      "          alone: 0.9030899869919435\n",
      "          think: 0.4259687322722811\n",
      "     boundaries: 0.9030899869919435\n",
      "      privately: 0.9030899869919435\n",
      "       position: 0.9030899869919435\n",
      "           this: 0.6020599913279624\n",
      "        startup: 0.9030899869919435\n",
      "             so: 0.6020599913279624\n",
      "         forest: 0.9030899869919435\n",
      "        nervous: 0.9030899869919435\n",
      "            may: 0.6020599913279624\n",
      "            20s: 0.9030899869919435\n",
      "        regrets: 0.6020599913279624\n",
      "          shame: 0.6020599913279624\n",
      "          early: 0.9030899869919435\n",
      "          loved: 0.9030899869919435\n",
      "           call: 0.9030899869919435\n",
      "           cant: 0.9030899869919435\n",
      "       approach: 0.9030899869919435\n",
      "         versus: 0.9030899869919435\n",
      "           draw: 0.9030899869919435\n",
      "          spoke: 0.6020599913279624\n",
      "          rates: 0.9030899869919435\n",
      "     vulnerable: 0.6020599913279624\n",
      "       yourself: 0.9030899869919435\n",
      "       wellness: 0.9030899869919435\n",
      "          there: 0.9030899869919435\n",
      "      important: 0.9030899869919435\n",
      "           real: 0.9030899869919435\n",
      "            way: 0.9030899869919435\n",
      "         spouse: 0.9030899869919435\n",
      "     supporters: 0.9030899869919435\n",
      "           felt: 0.4259687322722811\n",
      "          sales: 0.9030899869919435\n",
      "         needed: 0.6020599913279624\n",
      "          could: 0.6020599913279624\n",
      "         growth: 0.9030899869919435\n",
      "         school: 0.6020599913279624\n",
      "        smaller: 0.9030899869919435\n",
      "          great: 0.6020599913279624\n",
      "        evolved: 0.9030899869919435\n",
      "           grow: 0.9030899869919435\n",
      "         theyre: 0.9030899869919435\n",
      "        partner: 0.9030899869919435\n",
      "           best: 0.9030899869919435\n",
      "      untouched: 0.9030899869919435\n",
      "     technology: 0.9030899869919435\n",
      "        despite: 0.9030899869919435\n",
      "            try: 0.9030899869919435\n",
      "             im: 0.6020599913279624\n",
      "       practice: 0.9030899869919435\n",
      "           much: 0.4259687322722811\n",
      "          corny: 0.9030899869919435\n",
      "          years: 0.9030899869919435\n",
      "           zone: 0.6020599913279624\n",
      "        markers: 0.9030899869919435\n",
      "        beliefs: 0.9030899869919435\n",
      "        working: 0.6020599913279624\n",
      "         things: 0.6020599913279624\n",
      "          steps: 0.9030899869919435\n",
      "      companies: 0.9030899869919435\n",
      "         people: 0.6020599913279624\n",
      "            ive: 0.6020599913279624\n",
      "      fortunate: 0.9030899869919435\n",
      "          house: 0.9030899869919435\n",
      "       barriers: 0.9030899869919435\n",
      "          whats: 0.9030899869919435\n",
      "          water: 0.9030899869919435\n",
      "           wake: 0.9030899869919435\n",
      "      knowingly: 0.9030899869919435\n",
      "        quickly: 0.9030899869919435\n",
      "    crystallize: 0.9030899869919435\n",
      "          along: 0.9030899869919435\n",
      "          areas: 0.9030899869919435\n",
      "         minute: 0.9030899869919435\n",
      "            yet: 0.6020599913279624\n",
      "       emerging: 0.9030899869919435\n",
      "        whether: 0.9030899869919435\n",
      "           make: 0.6020599913279624\n",
      "         broken: 0.9030899869919435\n",
      "          blood: 0.9030899869919435\n",
      "             80: 0.9030899869919435\n",
      "           took: 0.9030899869919435\n",
      "          makes: 0.9030899869919435\n",
      "          quote: 0.9030899869919435\n",
      "        clearly: 0.9030899869919435\n",
      "          catch: 0.9030899869919435\n",
      "        resting: 0.9030899869919435\n",
      "      parenting: 0.9030899869919435\n",
      "      something: 0.6020599913279624\n",
      "     absolutely: 0.9030899869919435\n",
      "       somewhat: 0.9030899869919435\n",
      "       clinical: 0.6020599913279624\n",
      "      including: 0.9030899869919435\n",
      "        achieve: 0.9030899869919435\n",
      "           look: 0.9030899869919435\n",
      "         nearby: 0.9030899869919435\n",
      "      direction: 0.9030899869919435\n",
      "       pressure: 0.9030899869919435\n",
      "          admit: 0.6020599913279624\n",
      "          right: 0.6020599913279624\n",
      "        capture: 0.9030899869919435\n",
      "     confidence: 0.9030899869919435\n",
      "    essentially: 0.9030899869919435\n",
      "       anything: 0.9030899869919435\n",
      "      potential: 0.9030899869919435\n",
      "            own: 0.9030899869919435\n",
      "           told: 0.9030899869919435\n",
      "    effectively: 0.9030899869919435\n",
      "            men: 0.9030899869919435\n",
      "           past: 0.9030899869919435\n",
      "        setting: 0.9030899869919435\n",
      "      imagining: 0.9030899869919435\n",
      "       describe: 0.9030899869919435\n",
      "           ones: 0.6020599913279624\n",
      "          would: 0.9030899869919435\n",
      "        journal: 0.9030899869919435\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         nature: 0.9030899869919435\n",
      "      challenge: 0.6020599913279624\n",
      "       finished: 0.9030899869919435\n",
      "          voice: 0.9030899869919435\n",
      "          happy: 0.9030899869919435\n",
      "         course: 0.9030899869919435\n",
      "           been: 0.9030899869919435\n",
      "           hold: 0.9030899869919435\n",
      "        experts: 0.6020599913279624\n",
      "          shift: 0.9030899869919435\n",
      "         mental: 0.9030899869919435\n",
      "            sad: 0.9030899869919435\n",
      "           take: 0.9030899869919435\n",
      "         moment: 0.6020599913279624\n",
      "         really: 0.3010299956639812\n",
      "         future: 0.6020599913279624\n",
      "     struggling: 0.6020599913279624\n",
      "           that: 0.9030899869919435\n",
      "      grounding: 0.6020599913279624\n",
      "          thats: 0.6020599913279624\n",
      "         listen: 0.9030899869919435\n",
      "            low: 0.9030899869919435\n",
      "           tell: 0.6020599913279624\n",
      "            see: 0.9030899869919435\n",
      "          kinds: 0.6020599913279624\n",
      "        changes: 0.4259687322722811\n",
      "            key: 0.9030899869919435\n",
      "         seeing: 0.9030899869919435\n",
      "         mirror: 0.9030899869919435\n",
      "       although: 0.9030899869919435\n",
      "          paper: 0.9030899869919435\n",
      "          pitch: 0.9030899869919435\n",
      "          thing: 0.6020599913279624\n",
      "          brain: 0.9030899869919435\n",
      "        present: 0.9030899869919435\n",
      "    disciplines: 0.9030899869919435\n",
      "        reflect: 0.9030899869919435\n",
      "          takes: 0.9030899869919435\n",
      "         change: 0.3010299956639812\n",
      "       children: 0.9030899869919435\n",
      "           home: 0.9030899869919435\n",
      "          sense: 0.9030899869919435\n",
      "           keep: 0.9030899869919435\n",
      "          globe: 0.9030899869919435\n",
      "           talk: 0.9030899869919435\n",
      "    overwhelmed: 0.6020599913279624\n",
      " pharmaceutical: 0.9030899869919435\n",
      "   octogenarian: 0.9030899869919435\n",
      "     definitely: 0.9030899869919435\n",
      "          child: 0.9030899869919435\n",
      "           form: 0.9030899869919435\n",
      "          keeps: 0.9030899869919435\n",
      "           know: 0.05799194697768673\n",
      "     biological: 0.9030899869919435\n",
      "        shifted: 0.9030899869919435\n",
      "          based: 0.9030899869919435\n",
      "          guess: 0.9030899869919435\n",
      "           days: 0.9030899869919435\n",
      "          quiet: 0.9030899869919435\n",
      "      beginning: 0.9030899869919435\n",
      "       employed: 0.9030899869919435\n",
      "         within: 0.9030899869919435\n",
      "         little: 0.9030899869919435\n",
      "           well: 0.6020599913279624\n",
      "         almost: 0.9030899869919435\n",
      "      different: 0.6020599913279624\n",
      "            app: 0.9030899869919435\n",
      "          needs: 0.9030899869919435\n",
      "          small: 0.4259687322722811\n",
      "           time: 0.9030899869919435\n",
      "           were: 0.9030899869919435\n",
      "     strategies: 0.9030899869919435\n",
      "           part: 0.9030899869919435\n",
      "           many: 0.9030899869919435\n",
      "  consciousness: 0.9030899869919435\n",
      "       evermore: 0.9030899869919435\n",
      "         pretty: 0.9030899869919435\n"
     ]
    }
   ],
   "source": [
    "# 8.3 Manual calc of Inverse Document Frequency (IDF):\n",
    "\n",
    "print(\"IDF of: \")\n",
    " \n",
    "idf_all = {}\n",
    " \n",
    "for w in all_words_set:\n",
    "    k = 0    # number of documents in the corpus that contain this word\n",
    "     \n",
    "    for i in range(n_docs_all):\n",
    "        if w in corpus_all[i].split():\n",
    "            k += 1\n",
    "             \n",
    "    idf_all[w] =  np.log10(n_docs_all / k)\n",
    "     \n",
    "    print(f'{w:>15}: {idf_all[w]:>10}' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.3 ctd\n",
    "df_tf_all_idf = df_tf_all.copy()\n",
    " \n",
    "for w in all_words_set:\n",
    "    for i in range(n_docs_all):\n",
    "        df_tf_all_idf[w][i] = df_tf_all[w][i] * idf_all[w]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>side</th>\n",
       "      <th>work</th>\n",
       "      <th>incredible</th>\n",
       "      <th>behavior</th>\n",
       "      <th>detract</th>\n",
       "      <th>recognize</th>\n",
       "      <th>hard</th>\n",
       "      <th>with</th>\n",
       "      <th>traveled</th>\n",
       "      <th>metamorphosis</th>\n",
       "      <th>...</th>\n",
       "      <th>needs</th>\n",
       "      <th>small</th>\n",
       "      <th>time</th>\n",
       "      <th>were</th>\n",
       "      <th>strategies</th>\n",
       "      <th>part</th>\n",
       "      <th>many</th>\n",
       "      <th>consciousness</th>\n",
       "      <th>evermore</th>\n",
       "      <th>pretty</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.026958</td>\n",
       "      <td>0.006358</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013479</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013479</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013479</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.043004</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015242</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007621</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005392</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011432</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011432</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015242</td>\n",
       "      <td>0.011432</td>\n",
       "      <td>0.011432</td>\n",
       "      <td>0.011432</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007621</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005392</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009161</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009711</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009711</td>\n",
       "      <td>0.009711</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005392</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010784</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.023766</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.023766</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.023766</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 304 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       side      work  incredible  behavior   detract  recognize      hard  \\\n",
       "0  0.026958  0.006358    0.000000  0.000000  0.000000   0.000000  0.013479   \n",
       "1  0.000000  0.000000    0.000000  0.000000  0.000000   0.000000  0.000000   \n",
       "2  0.000000  0.000000    0.000000  0.000000  0.000000   0.000000  0.000000   \n",
       "3  0.000000  0.000000    0.015242  0.000000  0.000000   0.000000  0.000000   \n",
       "4  0.000000  0.000000    0.015242  0.011432  0.011432   0.011432  0.000000   \n",
       "5  0.000000  0.009161    0.000000  0.000000  0.000000   0.000000  0.000000   \n",
       "6  0.000000  0.005392    0.000000  0.000000  0.000000   0.000000  0.000000   \n",
       "7  0.000000  0.000000    0.000000  0.000000  0.000000   0.000000  0.000000   \n",
       "\n",
       "       with  traveled  metamorphosis  ...     needs     small      time  \\\n",
       "0  0.000000  0.000000       0.013479  ...  0.000000  0.000000  0.013479   \n",
       "1  0.000000  0.000000       0.000000  ...  0.000000  0.000000  0.000000   \n",
       "2  0.000000  0.000000       0.000000  ...  0.000000  0.000000  0.000000   \n",
       "3  0.007621  0.000000       0.000000  ...  0.000000  0.005392  0.000000   \n",
       "4  0.007621  0.000000       0.000000  ...  0.000000  0.005392  0.000000   \n",
       "5  0.000000  0.000000       0.000000  ...  0.000000  0.000000  0.000000   \n",
       "6  0.000000  0.000000       0.000000  ...  0.000000  0.010784  0.000000   \n",
       "7  0.000000  0.023766       0.000000  ...  0.023766  0.000000  0.000000   \n",
       "\n",
       "       were  strategies      part      many  consciousness  evermore    pretty  \n",
       "0  0.000000    0.000000  0.000000  0.000000       0.000000  0.000000  0.000000  \n",
       "1  0.000000    0.043004  0.000000  0.000000       0.000000  0.000000  0.000000  \n",
       "2  0.000000    0.000000  0.000000  0.000000       0.000000  0.000000  0.000000  \n",
       "3  0.011432    0.000000  0.000000  0.011432       0.000000  0.000000  0.000000  \n",
       "4  0.000000    0.000000  0.000000  0.000000       0.000000  0.000000  0.000000  \n",
       "5  0.000000    0.000000  0.009711  0.000000       0.009711  0.009711  0.000000  \n",
       "6  0.000000    0.000000  0.000000  0.000000       0.000000  0.000000  0.000000  \n",
       "7  0.000000    0.000000  0.000000  0.000000       0.000000  0.000000  0.023766  \n",
       "\n",
       "[8 rows x 304 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tf_all_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.4 apply sklearn TfidfVectorizer\n",
    "tr_idf_all_model  = TfidfVectorizer()\n",
    "tf_idf_all_vector = tr_idf_all_model.fit_transform(corpus_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.csr.csr_matrix'> (8, 304)\n"
     ]
    }
   ],
   "source": [
    "print(type(tf_idf_all_vector), tf_idf_all_vector.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words_set = tr_idf_all_model.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['20s', '80', 'absolutely', 'achieve', 'admit', 'again', 'almost', 'alone', 'along', 'although', 'another', 'anything', 'app', 'approach', 'areas', 'around', 'attracted', 'authentic', 'background', 'barriers', 'based', 'be', 'beautiful', 'been', 'beginning', 'behavior', 'beliefs', 'belong', 'best', 'biological', 'blood', 'boundaries', 'brain', 'broken', 'call', 'cant', 'capture', 'career', 'catch', 'challenge', 'challenging', 'change', 'changes', 'check', 'child', 'children', 'clearly', 'clinical', 'comfort', 'companies', 'confidence', 'consciousness', 'corny', 'could', 'course', 'crystallize', 'days', 'definitely', 'describe', 'despite', 'detract', 'detractors', 'dictation', 'different', 'dimensions', 'direction', 'disciplines', 'draw', 'early', 'effectively', 'emerging', 'employed', 'essentially', 'even', 'evermore', 'evolved', 'exceeding', 'experts', 'express', 'fact', 'family', 'feel', 'felt', 'finished', 'forest', 'form', 'fortunate', 'friends', 'from', 'fundamental', 'further', 'future', 'get', 'globe', 'goals', 'great', 'grounding', 'grow', 'growth', 'guess', 'happen', 'happy', 'hard', 'have', 'heart', 'hold', 'home', 'house', 'however', 'husband', 'ill', 'im', 'imagine', 'imagining', 'important', 'including', 'incredible', 'instructed', 'ive', 'journal', 'journaling', 'journey', 'keep', 'keeps', 'key', 'kids', 'kind', 'kinds', 'know', 'knowingly', 'like', 'listen', 'little', 'lived', 'look', 'loved', 'low', 'make', 'makes', 'many', 'markers', 'may', 'memos', 'men', 'mental', 'mentally', 'metamorphosis', 'minute', 'mirror', 'moment', 'motivation', 'much', 'multidimensional', 'myself', 'nature', 'nearby', 'need', 'needed', 'needs', 'nervous', 'no', 'number', 'octogenarian', 'old', 'one', 'ones', 'outside', 'overnight', 'overwhelmed', 'own', 'paper', 'parenting', 'part', 'partner', 'past', 'patience', 'people', 'pharmaceutical', 'pitch', 'place', 'placebo', 'position', 'potential', 'practice', 'present', 'pressure', 'pretty', 'privately', 'pushing', 'quickly', 'quiet', 'quote', 'rates', 'real', 'really', 'recognize', 'reflect', 'regrets', 'resting', 'right', 'sad', 'sales', 'say', 'saying', 'scared', 'scenery', 'school', 'see', 'seeing', 'self', 'sense', 'setting', 'several', 'shame', 'shift', 'shifted', 'side', 'simple', 'slightly', 'small', 'smaller', 'so', 'something', 'somewhat', 'sort', 'spoke', 'spouse', 'startup', 'steps', 'still', 'strategies', 'stream', 'stretching', 'struggling', 'studies', 'study', 'supporters', 'surround', 'take', 'takes', 'talk', 'technology', 'tell', 'that', 'thats', 'them', 'therapeutic', 'there', 'theyre', 'thing', 'things', 'think', 'this', 'thoughts', 'three', 'time', 'tips', 'today', 'told', 'took', 'tools', 'topic', 'transformation', 'traveled', 'trick', 'trusted', 'try', 'two', 'unquote', 'untouched', 'up', 'vast', 'versions', 'versus', 'voice', 'vulnerable', 'wake', 'want', 'water', 'way', 'ways', 'weeks', 'well', 'wellness', 'went', 'were', 'whatever', 'whats', 'whether', 'with', 'within', 'without', 'work', 'working', 'would', 'write', 'wrong', 'yeah', 'year', 'years', 'yet', 'you', 'yourself', 'zone']\n"
     ]
    }
   ],
   "source": [
    "print(all_words_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.4\n",
    "tf_idf_all_array = tf_idf_all_vector.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.4\n",
    "df_tf_all_idf = pd.DataFrame(tf_idf_all_array, columns = all_words_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>20s</th>\n",
       "      <th>80</th>\n",
       "      <th>absolutely</th>\n",
       "      <th>achieve</th>\n",
       "      <th>admit</th>\n",
       "      <th>again</th>\n",
       "      <th>almost</th>\n",
       "      <th>alone</th>\n",
       "      <th>along</th>\n",
       "      <th>although</th>\n",
       "      <th>...</th>\n",
       "      <th>would</th>\n",
       "      <th>write</th>\n",
       "      <th>wrong</th>\n",
       "      <th>yeah</th>\n",
       "      <th>year</th>\n",
       "      <th>years</th>\n",
       "      <th>yet</th>\n",
       "      <th>you</th>\n",
       "      <th>yourself</th>\n",
       "      <th>zone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.117758</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.117758</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.194513</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.194513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.312857</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.262198</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.099813</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.119097</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.119097</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.117379</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.098373</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.234758</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.098373</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.098373</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.092585</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.092585</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.077593</td>\n",
       "      <td>0.092585</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.113071</td>\n",
       "      <td>0.113071</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.113071</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.113071</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.094762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.175065</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.146718</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 304 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        20s        80  absolutely   achieve     admit     again    almost  \\\n",
       "0  0.000000  0.000000    0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "1  0.000000  0.000000    0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "2  0.000000  0.000000    0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "3  0.000000  0.000000    0.000000  0.000000  0.099813  0.000000  0.119097   \n",
       "4  0.000000  0.000000    0.117379  0.000000  0.098373  0.000000  0.000000   \n",
       "5  0.000000  0.000000    0.000000  0.000000  0.000000  0.092585  0.000000   \n",
       "6  0.113071  0.113071    0.000000  0.113071  0.000000  0.000000  0.000000   \n",
       "7  0.000000  0.000000    0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "      alone     along  although  ...     would     write     wrong      yeah  \\\n",
       "0  0.000000  0.000000  0.117758  ...  0.000000  0.000000  0.000000  0.000000   \n",
       "1  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.194513   \n",
       "2  0.000000  0.000000  0.000000  ...  0.000000  0.312857  0.000000  0.000000   \n",
       "3  0.000000  0.000000  0.000000  ...  0.119097  0.000000  0.000000  0.000000   \n",
       "4  0.000000  0.234758  0.000000  ...  0.000000  0.000000  0.000000  0.098373   \n",
       "5  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.092585  0.000000   \n",
       "6  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
       "7  0.175065  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "       year     years       yet       you  yourself      zone  \n",
       "0  0.000000  0.117758  0.000000  0.000000  0.000000  0.000000  \n",
       "1  0.000000  0.000000  0.000000  0.000000  0.000000  0.194513  \n",
       "2  0.000000  0.000000  0.000000  0.262198  0.000000  0.000000  \n",
       "3  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "4  0.000000  0.000000  0.098373  0.000000  0.000000  0.000000  \n",
       "5  0.000000  0.000000  0.000000  0.077593  0.092585  0.000000  \n",
       "6  0.113071  0.000000  0.000000  0.000000  0.000000  0.094762  \n",
       "7  0.000000  0.000000  0.146718  0.000000  0.000000  0.000000  \n",
       "\n",
       "[8 rows x 304 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tf_all_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.4.1 distill that out\n",
    "cols = df_tf_all_idf.columns\n",
    "bt = df_tf_all_idf.apply(lambda x: x > 0)\n",
    "final_shape_all = [bt.apply(lambda x: list(cols[x.values]), axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0    [although, based, call, change, clearly, clini...\n",
      "1    [approach, barriers, change, changes, comfort,...\n",
      "2    [been, catch, confidence, happen, know, like, ...\n",
      "3    [admit, almost, another, areas, authentic, bra...\n",
      "4    [absolutely, admit, along, another, authentic,...\n",
      "5    [again, anything, app, attracted, be, beliefs,...\n",
      "6    [20s, 80, achieve, around, beginning, belong, ...\n",
      "7    [alone, beautiful, children, corny, could, for...\n",
      "dtype: object]\n"
     ]
    }
   ],
   "source": [
    "print(final_shape_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9.0 Nouns\n",
    "    # make a corpus from all 8 files\n",
    "    # make a word-set (vocab of unique words) of the corpus\n",
    "    # manual Term Frequeny calc\n",
    "    # manual inverse document frequency (IDF) calc\n",
    "    # apply sklearn TfidfVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['people', 'time', 'change', 'transformation', 'metamorphosis', 'call', 'future', 'right', 'past', 'self', 'self', 'versions', 'people', 'journey', 'kinds', 'years', 'side', 'practice', 'pharmaceutical', 'side', 'pharmaceutical', 'companies', 'work', 'technology', 'technology', 'startup', 'companies'], ['strategies', 'talk', 'minute', 'change', 'changes', 'zone', 'barriers'], ['write', 'confidence'], ['experts', 'motivation', 'tips', 'ways', 'number', 'friends', 'family', 'change', 'challenge', 'admit', 'regrets', 'shame', 'changes', 'future', 'days', 'areas', 'know', 'husband', 'patience', 'ones', 'areas', 'child', 'school', 'dimensions', 'kids', 'kinds', 'things', 'brain'], ['supporters', 'detractors', 'thing', 'recognize', 'today', 'sense', 'direction', 'growth', 'goals', 'wellness', 'background', 'behavior', 'change', 'experts', 'motivation', 'tips', 'ways', 'number', 'friends', 'family', 'change', 'challenge', 'admit', 'regrets', 'shame', 'changes', 'make'], ['form', 'school', 'paper', 'form', 'dictation', 'fact', 'part', 'app', 'sales', 'pitch', 'way', 'moment', 'paper', 'voice', 'memos', 'tools', 'stream', 'consciousness', 'thoughts', 'topic', 'thing', 'thing', 'challenging', 'thing', 'thing', 'reflect', 'crystallize', 'position', 'hold', 'beliefs', 'shift', 'career', 'work', 'home'], ['comfort', 'zone', 'boundaries', 'steps', 'goals', 'boundaries', 'things', 'placebo', 'studies', 'people', 'year', 'men', 'weeks', '20s', 'mirror', 'moment', 'course', 'study', 'blood', 'pressure', 'heart', 'rates', 'markers'], ['globe', 'place', 'water', 'forest', 'nature', 'ones', 'children', 'friends', 'scenery']]\n"
     ]
    }
   ],
   "source": [
    "print(all_nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['form', 'school', 'paper', 'form', 'dictation', 'fact', 'part', 'app', 'sales', 'pitch', 'way', 'moment', 'paper', 'voice', 'memos', 'tools', 'stream', 'consciousness', 'thoughts', 'topic', 'thing', 'thing', 'challenging', 'thing', 'thing', 'reflect', 'crystallize', 'position', 'hold', 'beliefs', 'shift', 'career', 'work', 'home']\n"
     ]
    }
   ],
   "source": [
    "print(all_nouns[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9.1 make a corpus from all 8 files\n",
    "\n",
    "# https://www.programiz.com/python-programming/examples/flatten-nested-list\n",
    "\n",
    "# how to put all_nouns into right format and pass through? It needs to be a list of strings not a list of lists. Inner \n",
    "# list must be joined.\n",
    "\n",
    "corpus_nouns = [\" \".join(lst) for lst in all_nouns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['people time change transformation metamorphosis call future right past self self versions people journey kinds years side practice pharmaceutical side pharmaceutical companies work technology technology startup companies', 'strategies talk minute change changes zone barriers', 'write confidence', 'experts motivation tips ways number friends family change challenge admit regrets shame changes future days areas know husband patience ones areas child school dimensions kids kinds things brain', 'supporters detractors thing recognize today sense direction growth goals wellness background behavior change experts motivation tips ways number friends family change challenge admit regrets shame changes make', 'form school paper form dictation fact part app sales pitch way moment paper voice memos tools stream consciousness thoughts topic thing thing challenging thing thing reflect crystallize position hold beliefs shift career work home', 'comfort zone boundaries steps goals boundaries things placebo studies people year men weeks 20s mirror moment course study blood pressure heart rates markers', 'globe place water forest nature ones children friends scenery']\n"
     ]
    }
   ],
   "source": [
    "print(corpus_nouns) # each sublist needs to be a string not a list, len should = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9.2 create a word set/vocab for the corpus\n",
    "noun_words_set = set()\n",
    " \n",
    "for doc in corpus_nouns:\n",
    "    words_nouns = doc.split(' ')\n",
    "    noun_words_set = noun_words_set.union(set(words_nouns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in the corpus: 115\n"
     ]
    }
   ],
   "source": [
    "print('Number of words in the corpus:',len(noun_words_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The words in the corpus: \n",
      " {'side', 'work', 'moment', 'comfort', 'behavior', 'practice', 'recognize', 'future', 'fact', 'dimensions', 'metamorphosis', 'study', 'self', 'tools', 'dictation', 'topic', 'kinds', 'years', 'changes', 'zone', 'markers', 'beliefs', 'things', 'steps', 'mirror', 'companies', 'scenery', 'people', 'family', 'place', 'barriers', 'water', 'paper', 'pitch', 'thing', 'brain', 'thoughts', 'motivation', 'number', 'kids', 'reflect', 'crystallize', 'patience', 'change', 'detractors', 'year', 'goals', 'children', 'studies', 'areas', 'home', 'boundaries', 'minute', 'today', 'position', 'versions', 'startup', 'sense', 'globe', 'make', 'forest', '20s', 'challenging', 'blood', 'regrets', 'shame', 'talk', 'journey', 'call', 'pharmaceutical', 'memos', 'child', 'background', 'career', 'friends', 'form', 'transformation', 'know', 'ways', 'rates', 'direction', 'days', 'pressure', 'admit', 'wellness', 'right', 'tips', 'heart', 'confidence', 'way', 'men', 'placebo', 'past', 'write', 'ones', 'supporters', 'nature', 'sales', 'challenge', 'growth', 'app', 'school', 'time', 'husband', 'strategies', 'voice', 'part', 'course', 'stream', 'consciousness', 'hold', 'experts', 'shift', 'weeks', 'technology'}\n"
     ]
    }
   ],
   "source": [
    "print('The words in the corpus: \\n', noun_words_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9.3 manual Term Frequeny calc\n",
    "\n",
    "n_docs_nouns = len(corpus_nouns)         #·Number of documents in the corpus\n",
    "n_words_set_nouns = len(noun_words_set) #·Number of unique words in the\n",
    " \n",
    "df_tf_nouns = pd.DataFrame(np.zeros((n_docs_nouns, n_words_set_nouns)), columns=noun_words_set)\n",
    " \n",
    "# Compute Term Frequency (TF)\n",
    "for i in range(n_docs_nouns):\n",
    "    words = corpus_nouns[i].split(' ') # Words in the document\n",
    "    for w in words:\n",
    "        df_tf_nouns[w][i] = df_tf_nouns[w][i] + (1 / len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>side</th>\n",
       "      <th>work</th>\n",
       "      <th>moment</th>\n",
       "      <th>comfort</th>\n",
       "      <th>behavior</th>\n",
       "      <th>practice</th>\n",
       "      <th>recognize</th>\n",
       "      <th>future</th>\n",
       "      <th>fact</th>\n",
       "      <th>dimensions</th>\n",
       "      <th>...</th>\n",
       "      <th>voice</th>\n",
       "      <th>part</th>\n",
       "      <th>course</th>\n",
       "      <th>stream</th>\n",
       "      <th>consciousness</th>\n",
       "      <th>hold</th>\n",
       "      <th>experts</th>\n",
       "      <th>shift</th>\n",
       "      <th>weeks</th>\n",
       "      <th>technology</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.074074</td>\n",
       "      <td>0.037037</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.037037</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.037037</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.074074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.035714</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.035714</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.035714</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.037037</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.037037</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.037037</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.029412</td>\n",
       "      <td>0.029412</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.029412</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.029412</td>\n",
       "      <td>0.029412</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.029412</td>\n",
       "      <td>0.029412</td>\n",
       "      <td>0.029412</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.029412</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 115 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       side      work    moment   comfort  behavior  practice  recognize  \\\n",
       "0  0.074074  0.037037  0.000000  0.000000  0.000000  0.037037   0.000000   \n",
       "1  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   0.000000   \n",
       "2  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   0.000000   \n",
       "3  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   0.000000   \n",
       "4  0.000000  0.000000  0.000000  0.000000  0.037037  0.000000   0.037037   \n",
       "5  0.000000  0.029412  0.029412  0.000000  0.000000  0.000000   0.000000   \n",
       "6  0.000000  0.000000  0.043478  0.043478  0.000000  0.000000   0.000000   \n",
       "7  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   0.000000   \n",
       "\n",
       "     future      fact  dimensions  ...     voice      part    course  \\\n",
       "0  0.037037  0.000000    0.000000  ...  0.000000  0.000000  0.000000   \n",
       "1  0.000000  0.000000    0.000000  ...  0.000000  0.000000  0.000000   \n",
       "2  0.000000  0.000000    0.000000  ...  0.000000  0.000000  0.000000   \n",
       "3  0.035714  0.000000    0.035714  ...  0.000000  0.000000  0.000000   \n",
       "4  0.000000  0.000000    0.000000  ...  0.000000  0.000000  0.000000   \n",
       "5  0.000000  0.029412    0.000000  ...  0.029412  0.029412  0.000000   \n",
       "6  0.000000  0.000000    0.000000  ...  0.000000  0.000000  0.043478   \n",
       "7  0.000000  0.000000    0.000000  ...  0.000000  0.000000  0.000000   \n",
       "\n",
       "     stream  consciousness      hold   experts     shift     weeks  technology  \n",
       "0  0.000000       0.000000  0.000000  0.000000  0.000000  0.000000    0.074074  \n",
       "1  0.000000       0.000000  0.000000  0.000000  0.000000  0.000000    0.000000  \n",
       "2  0.000000       0.000000  0.000000  0.000000  0.000000  0.000000    0.000000  \n",
       "3  0.000000       0.000000  0.000000  0.035714  0.000000  0.000000    0.000000  \n",
       "4  0.000000       0.000000  0.000000  0.037037  0.000000  0.000000    0.000000  \n",
       "5  0.029412       0.029412  0.029412  0.000000  0.029412  0.000000    0.000000  \n",
       "6  0.000000       0.000000  0.000000  0.000000  0.000000  0.043478    0.000000  \n",
       "7  0.000000       0.000000  0.000000  0.000000  0.000000  0.000000    0.000000  \n",
       "\n",
       "[8 rows x 115 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The dataframe above shows we have a column for each word and a row for each document.\n",
    "#This shows the frequency of each word in each document.\n",
    "df_tf_nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IDF of: \n",
      "           side: 0.9030899869919435\n",
      "           work: 0.6020599913279624\n",
      "         moment: 0.6020599913279624\n",
      "        comfort: 0.9030899869919435\n",
      "       behavior: 0.9030899869919435\n",
      "       practice: 0.9030899869919435\n",
      "      recognize: 0.9030899869919435\n",
      "         future: 0.6020599913279624\n",
      "           fact: 0.9030899869919435\n",
      "     dimensions: 0.9030899869919435\n",
      "  metamorphosis: 0.9030899869919435\n",
      "          study: 0.9030899869919435\n",
      "           self: 0.9030899869919435\n",
      "          tools: 0.9030899869919435\n",
      "      dictation: 0.9030899869919435\n",
      "          topic: 0.9030899869919435\n",
      "          kinds: 0.6020599913279624\n",
      "          years: 0.9030899869919435\n",
      "        changes: 0.4259687322722811\n",
      "           zone: 0.6020599913279624\n",
      "        markers: 0.9030899869919435\n",
      "        beliefs: 0.9030899869919435\n",
      "         things: 0.6020599913279624\n",
      "          steps: 0.9030899869919435\n",
      "         mirror: 0.9030899869919435\n",
      "      companies: 0.9030899869919435\n",
      "        scenery: 0.9030899869919435\n",
      "         people: 0.6020599913279624\n",
      "         family: 0.6020599913279624\n",
      "          place: 0.9030899869919435\n",
      "       barriers: 0.9030899869919435\n",
      "          water: 0.9030899869919435\n",
      "          paper: 0.9030899869919435\n",
      "          pitch: 0.9030899869919435\n",
      "          thing: 0.6020599913279624\n",
      "          brain: 0.9030899869919435\n",
      "       thoughts: 0.9030899869919435\n",
      "     motivation: 0.6020599913279624\n",
      "         number: 0.6020599913279624\n",
      "           kids: 0.9030899869919435\n",
      "        reflect: 0.9030899869919435\n",
      "    crystallize: 0.9030899869919435\n",
      "       patience: 0.9030899869919435\n",
      "         change: 0.3010299956639812\n",
      "     detractors: 0.9030899869919435\n",
      "           year: 0.9030899869919435\n",
      "          goals: 0.6020599913279624\n",
      "       children: 0.9030899869919435\n",
      "        studies: 0.9030899869919435\n",
      "          areas: 0.9030899869919435\n",
      "           home: 0.9030899869919435\n",
      "     boundaries: 0.9030899869919435\n",
      "         minute: 0.9030899869919435\n",
      "          today: 0.9030899869919435\n",
      "       position: 0.9030899869919435\n",
      "       versions: 0.9030899869919435\n",
      "        startup: 0.9030899869919435\n",
      "          sense: 0.9030899869919435\n",
      "          globe: 0.9030899869919435\n",
      "           make: 0.9030899869919435\n",
      "         forest: 0.9030899869919435\n",
      "            20s: 0.9030899869919435\n",
      "    challenging: 0.9030899869919435\n",
      "          blood: 0.9030899869919435\n",
      "        regrets: 0.6020599913279624\n",
      "          shame: 0.6020599913279624\n",
      "           talk: 0.9030899869919435\n",
      "        journey: 0.9030899869919435\n",
      "           call: 0.9030899869919435\n",
      " pharmaceutical: 0.9030899869919435\n",
      "          memos: 0.9030899869919435\n",
      "          child: 0.9030899869919435\n",
      "     background: 0.9030899869919435\n",
      "         career: 0.9030899869919435\n",
      "        friends: 0.4259687322722811\n",
      "           form: 0.9030899869919435\n",
      " transformation: 0.9030899869919435\n",
      "           know: 0.9030899869919435\n",
      "           ways: 0.6020599913279624\n",
      "          rates: 0.9030899869919435\n",
      "      direction: 0.9030899869919435\n",
      "           days: 0.9030899869919435\n",
      "       pressure: 0.9030899869919435\n",
      "          admit: 0.6020599913279624\n",
      "       wellness: 0.9030899869919435\n",
      "          right: 0.9030899869919435\n",
      "           tips: 0.6020599913279624\n",
      "          heart: 0.9030899869919435\n",
      "     confidence: 0.9030899869919435\n",
      "            way: 0.9030899869919435\n",
      "            men: 0.9030899869919435\n",
      "        placebo: 0.9030899869919435\n",
      "           past: 0.9030899869919435\n",
      "          write: 0.9030899869919435\n",
      "           ones: 0.6020599913279624\n",
      "     supporters: 0.9030899869919435\n",
      "         nature: 0.9030899869919435\n",
      "          sales: 0.9030899869919435\n",
      "      challenge: 0.6020599913279624\n",
      "         growth: 0.9030899869919435\n",
      "            app: 0.9030899869919435\n",
      "         school: 0.6020599913279624\n",
      "           time: 0.9030899869919435\n",
      "        husband: 0.9030899869919435\n",
      "     strategies: 0.9030899869919435\n",
      "          voice: 0.9030899869919435\n",
      "           part: 0.9030899869919435\n",
      "         course: 0.9030899869919435\n",
      "         stream: 0.9030899869919435\n",
      "  consciousness: 0.9030899869919435\n",
      "           hold: 0.9030899869919435\n",
      "        experts: 0.6020599913279624\n",
      "          shift: 0.9030899869919435\n",
      "          weeks: 0.9030899869919435\n",
      "     technology: 0.9030899869919435\n"
     ]
    }
   ],
   "source": [
    "#    # 9.4 manual inverse document frequency (IDF) calc\n",
    "print(\"IDF of: \")\n",
    " \n",
    "idf_nouns = {}\n",
    " \n",
    "for w in noun_words_set:\n",
    "    k = 0    # number of documents in the corpus that contain this word\n",
    "     \n",
    "    for i in range(n_docs_nouns):\n",
    "        if w in corpus_nouns[i].split():\n",
    "            k += 1\n",
    "             \n",
    "    idf_nouns[w] =  np.log10(n_docs_nouns / k)\n",
    "     \n",
    "    print(f'{w:>15}: {idf_nouns[w]:>10}' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9.4 ctd\n",
    "df_tf_nouns_idf = df_tf_nouns.copy()\n",
    " \n",
    "for w in noun_words_set:\n",
    "    for i in range(n_docs_nouns):\n",
    "        df_tf_nouns_idf[w][i] = df_tf_nouns[w][i] * idf_nouns[w]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>side</th>\n",
       "      <th>work</th>\n",
       "      <th>moment</th>\n",
       "      <th>comfort</th>\n",
       "      <th>behavior</th>\n",
       "      <th>practice</th>\n",
       "      <th>recognize</th>\n",
       "      <th>future</th>\n",
       "      <th>fact</th>\n",
       "      <th>dimensions</th>\n",
       "      <th>...</th>\n",
       "      <th>voice</th>\n",
       "      <th>part</th>\n",
       "      <th>course</th>\n",
       "      <th>stream</th>\n",
       "      <th>consciousness</th>\n",
       "      <th>hold</th>\n",
       "      <th>experts</th>\n",
       "      <th>shift</th>\n",
       "      <th>weeks</th>\n",
       "      <th>technology</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.066896</td>\n",
       "      <td>0.022299</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.033448</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.022299</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.066896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.021502</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.032253</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.021502</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.033448</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.033448</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.022299</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.017708</td>\n",
       "      <td>0.017708</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.026561</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.026561</td>\n",
       "      <td>0.026561</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.026561</td>\n",
       "      <td>0.026561</td>\n",
       "      <td>0.026561</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.026561</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.026177</td>\n",
       "      <td>0.039265</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.039265</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.039265</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 115 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       side      work    moment   comfort  behavior  practice  recognize  \\\n",
       "0  0.066896  0.022299  0.000000  0.000000  0.000000  0.033448   0.000000   \n",
       "1  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   0.000000   \n",
       "2  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   0.000000   \n",
       "3  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   0.000000   \n",
       "4  0.000000  0.000000  0.000000  0.000000  0.033448  0.000000   0.033448   \n",
       "5  0.000000  0.017708  0.017708  0.000000  0.000000  0.000000   0.000000   \n",
       "6  0.000000  0.000000  0.026177  0.039265  0.000000  0.000000   0.000000   \n",
       "7  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   0.000000   \n",
       "\n",
       "     future      fact  dimensions  ...     voice      part    course  \\\n",
       "0  0.022299  0.000000    0.000000  ...  0.000000  0.000000  0.000000   \n",
       "1  0.000000  0.000000    0.000000  ...  0.000000  0.000000  0.000000   \n",
       "2  0.000000  0.000000    0.000000  ...  0.000000  0.000000  0.000000   \n",
       "3  0.021502  0.000000    0.032253  ...  0.000000  0.000000  0.000000   \n",
       "4  0.000000  0.000000    0.000000  ...  0.000000  0.000000  0.000000   \n",
       "5  0.000000  0.026561    0.000000  ...  0.026561  0.026561  0.000000   \n",
       "6  0.000000  0.000000    0.000000  ...  0.000000  0.000000  0.039265   \n",
       "7  0.000000  0.000000    0.000000  ...  0.000000  0.000000  0.000000   \n",
       "\n",
       "     stream  consciousness      hold   experts     shift     weeks  technology  \n",
       "0  0.000000       0.000000  0.000000  0.000000  0.000000  0.000000    0.066896  \n",
       "1  0.000000       0.000000  0.000000  0.000000  0.000000  0.000000    0.000000  \n",
       "2  0.000000       0.000000  0.000000  0.000000  0.000000  0.000000    0.000000  \n",
       "3  0.000000       0.000000  0.000000  0.021502  0.000000  0.000000    0.000000  \n",
       "4  0.000000       0.000000  0.000000  0.022299  0.000000  0.000000    0.000000  \n",
       "5  0.026561       0.026561  0.026561  0.000000  0.026561  0.000000    0.000000  \n",
       "6  0.000000       0.000000  0.000000  0.000000  0.000000  0.039265    0.000000  \n",
       "7  0.000000       0.000000  0.000000  0.000000  0.000000  0.000000    0.000000  \n",
       "\n",
       "[8 rows x 115 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tf_nouns_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9.5 apply sklearn TfidfVectorizer\n",
    "\n",
    "tr_idf_nouns_model  = TfidfVectorizer()\n",
    "tf_idf_nouns_vector = tr_idf_nouns_model.fit_transform(corpus_nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.csr.csr_matrix'> (8, 115)\n"
     ]
    }
   ],
   "source": [
    "print(type(tf_idf_nouns_vector), tf_idf_nouns_vector.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9.5 continued\n",
    "noun_words_set = tr_idf_nouns_model.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['20s', 'admit', 'app', 'areas', 'background', 'barriers', 'behavior', 'beliefs', 'blood', 'boundaries', 'brain', 'call', 'career', 'challenge', 'challenging', 'change', 'changes', 'child', 'children', 'comfort', 'companies', 'confidence', 'consciousness', 'course', 'crystallize', 'days', 'detractors', 'dictation', 'dimensions', 'direction', 'experts', 'fact', 'family', 'forest', 'form', 'friends', 'future', 'globe', 'goals', 'growth', 'heart', 'hold', 'home', 'husband', 'journey', 'kids', 'kinds', 'know', 'make', 'markers', 'memos', 'men', 'metamorphosis', 'minute', 'mirror', 'moment', 'motivation', 'nature', 'number', 'ones', 'paper', 'part', 'past', 'patience', 'people', 'pharmaceutical', 'pitch', 'place', 'placebo', 'position', 'practice', 'pressure', 'rates', 'recognize', 'reflect', 'regrets', 'right', 'sales', 'scenery', 'school', 'self', 'sense', 'shame', 'shift', 'side', 'startup', 'steps', 'strategies', 'stream', 'studies', 'study', 'supporters', 'talk', 'technology', 'thing', 'things', 'thoughts', 'time', 'tips', 'today', 'tools', 'topic', 'transformation', 'versions', 'voice', 'water', 'way', 'ways', 'weeks', 'wellness', 'work', 'write', 'year', 'years', 'zone']\n"
     ]
    }
   ],
   "source": [
    "print(noun_words_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9.5 continued\n",
    "tf_idf_nouns_array = tf_idf_nouns_vector.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9.5 continued\n",
    "df_tf_nouns_idf = pd.DataFrame(tf_idf_nouns_array, columns = noun_words_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>20s</th>\n",
       "      <th>admit</th>\n",
       "      <th>app</th>\n",
       "      <th>areas</th>\n",
       "      <th>background</th>\n",
       "      <th>barriers</th>\n",
       "      <th>behavior</th>\n",
       "      <th>beliefs</th>\n",
       "      <th>blood</th>\n",
       "      <th>boundaries</th>\n",
       "      <th>...</th>\n",
       "      <th>water</th>\n",
       "      <th>way</th>\n",
       "      <th>ways</th>\n",
       "      <th>weeks</th>\n",
       "      <th>wellness</th>\n",
       "      <th>work</th>\n",
       "      <th>write</th>\n",
       "      <th>year</th>\n",
       "      <th>years</th>\n",
       "      <th>zone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.139066</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.165934</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.421546</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.353288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.171130</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.408387</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.171130</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.178344</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.212801</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.212801</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.178344</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.212801</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.150168</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.150168</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.150168</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.125852</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.206232</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.206232</td>\n",
       "      <td>0.412464</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.206232</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.206232</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.172839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.348676</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 115 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        20s     admit       app     areas  background  barriers  behavior  \\\n",
       "0  0.000000  0.000000  0.000000  0.000000    0.000000  0.000000  0.000000   \n",
       "1  0.000000  0.000000  0.000000  0.000000    0.000000  0.421546  0.000000   \n",
       "2  0.000000  0.000000  0.000000  0.000000    0.000000  0.000000  0.000000   \n",
       "3  0.000000  0.171130  0.000000  0.408387    0.000000  0.000000  0.000000   \n",
       "4  0.000000  0.178344  0.000000  0.000000    0.212801  0.000000  0.212801   \n",
       "5  0.000000  0.000000  0.150168  0.000000    0.000000  0.000000  0.000000   \n",
       "6  0.206232  0.000000  0.000000  0.000000    0.000000  0.000000  0.000000   \n",
       "7  0.000000  0.000000  0.000000  0.000000    0.000000  0.000000  0.000000   \n",
       "\n",
       "    beliefs     blood  boundaries  ...     water       way      ways  \\\n",
       "0  0.000000  0.000000    0.000000  ...  0.000000  0.000000  0.000000   \n",
       "1  0.000000  0.000000    0.000000  ...  0.000000  0.000000  0.000000   \n",
       "2  0.000000  0.000000    0.000000  ...  0.000000  0.000000  0.000000   \n",
       "3  0.000000  0.000000    0.000000  ...  0.000000  0.000000  0.171130   \n",
       "4  0.000000  0.000000    0.000000  ...  0.000000  0.000000  0.178344   \n",
       "5  0.150168  0.000000    0.000000  ...  0.000000  0.150168  0.000000   \n",
       "6  0.000000  0.206232    0.412464  ...  0.000000  0.000000  0.000000   \n",
       "7  0.000000  0.000000    0.000000  ...  0.348676  0.000000  0.000000   \n",
       "\n",
       "      weeks  wellness      work     write      year     years      zone  \n",
       "0  0.000000  0.000000  0.139066  0.000000  0.000000  0.165934  0.000000  \n",
       "1  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.353288  \n",
       "2  0.000000  0.000000  0.000000  0.707107  0.000000  0.000000  0.000000  \n",
       "3  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "4  0.000000  0.212801  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "5  0.000000  0.000000  0.125852  0.000000  0.000000  0.000000  0.000000  \n",
       "6  0.206232  0.000000  0.000000  0.000000  0.206232  0.000000  0.172839  \n",
       "7  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "\n",
       "[8 rows x 115 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tf_nouns_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9.5.1 final shape\n",
    "cols = df_tf_nouns_idf.columns\n",
    "bt = df_tf_nouns_idf.apply(lambda x: x > 0)\n",
    "final_shape_nouns = [bt.apply(lambda x: list(cols[x.values]), axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0    [call, change, companies, future, journey, kin...\n",
      "1    [barriers, change, changes, minute, strategies...\n",
      "2                                  [confidence, write]\n",
      "3    [admit, areas, brain, challenge, change, chang...\n",
      "4    [admit, background, behavior, challenge, chang...\n",
      "5    [app, beliefs, career, challenging, consciousn...\n",
      "6    [20s, blood, boundaries, comfort, course, goal...\n",
      "7    [children, forest, friends, globe, nature, one...\n",
      "dtype: object]\n"
     ]
    }
   ],
   "source": [
    "print(final_shape_nouns) #might be better as dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10.0 Verbs\n",
    "    # 10.1 make a corpus from all 8 files\n",
    "    # 10.2 make a word-set (vocab of unique words) of the corpus\n",
    "    # 10.3 manual Term Frequeny calc\n",
    "    # 10.4 manual inverse document frequency (IDF) calc\n",
    "    # 10.5 apply sklearn TfidfVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[say, seeing, imagining, want, listen, think, know, went, disciplines, know, working, partner, ve, shifted, emerging, based], [employed, know, re, guess], [know, takes, catch, happen, s], [went, grounding, check, spoke, trusted, struggling, feel, know, felt, needed, make, felt, would, evolved, know, m, working, including, tell, working, know, get, draw, know, describe], [think, may, detract, makes, know, ca, grow, need, know, know, background, went, grounding, check, spoke, trusted, struggling, feel, know, felt, needed], [s, attracted, m, felt, express, work, capture, may, want, need, need, think, setting, parenting, setting], [keeps, stretching, feel, achieve, pushing, know, exceeding, say, take, keep, pushing, trick, told, tell, work, instructed, try, look, try, could, went, wake, beginning, took, resting], [ve, lived, traveled, needs, s, know, know, loved, could, know, spouse]]\n"
     ]
    }
   ],
   "source": [
    "print(all_verbs) #list of list of spacy tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_verbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_verbs[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_verbs[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(all_verbs[5][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10.1.1 extract stems from verbs to normalise vs tense\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "all_verb_stems = []\n",
    "\n",
    "for doc in all_verbs:\n",
    "    for item in doc:\n",
    "        all_verb_stems.append(stemmer.stem(str(item)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_verbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(all_verbs[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(all_verbs[5][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[say, seeing, imagining, want, listen, think, know, went, disciplines, know, working, partner, ve, shifted, emerging, based]\n",
      "[employed, know, re, guess]\n",
      "[know, takes, catch, happen, s]\n",
      "[went, grounding, check, spoke, trusted, struggling, feel, know, felt, needed, make, felt, would, evolved, know, m, working, including, tell, working, know, get, draw, know, describe]\n",
      "[think, may, detract, makes, know, ca, grow, need, know, know, background, went, grounding, check, spoke, trusted, struggling, feel, know, felt, needed]\n",
      "[s, attracted, m, felt, express, work, capture, may, want, need, need, think, setting, parenting, setting]\n",
      "[keeps, stretching, feel, achieve, pushing, know, exceeding, say, take, keep, pushing, trick, told, tell, work, instructed, try, look, try, could, went, wake, beginning, took, resting]\n",
      "[ve, lived, traveled, needs, s, know, know, loved, could, know, spouse]\n"
     ]
    }
   ],
   "source": [
    "# 10.1.2\n",
    "all_verbs_stemmerFeed = [[] for x in range(list_len)]\n",
    "\n",
    "for i in range(len(all_verbs)):\n",
    "    print(all_verbs[i])\n",
    "    all_verbs_stemmerFeed[i] = [vs for vs in all_verbs[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10.1.3\n",
    "stemmer = PorterStemmer()\n",
    "all_verbStems = [[] for x in range(list_len)]\n",
    "\n",
    "for i in range(len(all_verbs)):\n",
    "    all_verbStems[i] = [stemmer.stem(str(vs)) for vs in all_verbs_stemmerFeed[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['say', 'see', 'imagin', 'want', 'listen', 'think', 'know', 'went', 'disciplin', 'know', 'work', 'partner', 've', 'shift', 'emerg', 'base', 'employ', 'know', 're', 'guess', 'know', 'take', 'catch', 'happen', 's', 'went', 'ground', 'check', 'spoke', 'trust', 'struggl', 'feel', 'know', 'felt', 'need', 'make', 'felt', 'would', 'evolv', 'know', 'm', 'work', 'includ', 'tell', 'work', 'know', 'get', 'draw', 'know', 'describ', 'think', 'may', 'detract', 'make', 'know', 'ca', 'grow', 'need', 'know', 'know', 'background', 'went', 'ground', 'check', 'spoke', 'trust', 'struggl', 'feel', 'know', 'felt', 'need', 's', 'attract', 'm', 'felt', 'express', 'work', 'captur', 'may', 'want', 'need', 'need', 'think', 'set', 'parent', 'set', 'keep', 'stretch', 'feel', 'achiev', 'push', 'know', 'exceed', 'say', 'take', 'keep', 'push', 'trick', 'told', 'tell', 'work', 'instruct', 'tri', 'look', 'tri', 'could', 'went', 'wake', 'begin', 'took', 'rest', 've', 'live', 'travel', 'need', 's', 'know', 'know', 'love', 'could', 'know', 'spous']\n"
     ]
    }
   ],
   "source": [
    "print(all_verb_stems) # WRONG - still needs to be subdivided into docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['s', 'attract', 'm', 'felt', 'express', 'work', 'captur', 'may', 'want', 'need', 'need', 'think', 'set', 'parent', 'set']\n"
     ]
    }
   ],
   "source": [
    "print(all_verbStems[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_verbStems) #all_verbStems is the corpus for verbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10.2 create a word set/vocab for the verbs' corpus\n",
    "# Make list of unique vocab lists for (verb_stems, nouns, adjs, advs) - can be done with set() and .union - how?\n",
    "unique_verbStems_list = []\n",
    "\n",
    "for i in range(len(all_verbStems)):\n",
    "    for item in all_verbStems[i]:\n",
    "        if item in unique_verbStems_list:\n",
    "            continue\n",
    "        else:\n",
    "            unique_verbStems_list.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['say', 'see', 'imagin', 'want', 'listen', 'think', 'know', 'went', 'disciplin', 'work', 'partner', 've', 'shift', 'emerg', 'base', 'employ', 're', 'guess', 'take', 'catch', 'happen', 's', 'ground', 'check', 'spoke', 'trust', 'struggl', 'feel', 'felt', 'need', 'make', 'would', 'evolv', 'm', 'includ', 'tell', 'get', 'draw', 'describ', 'may', 'detract', 'ca', 'grow', 'background', 'attract', 'express', 'captur', 'set', 'parent', 'keep', 'stretch', 'achiev', 'push', 'exceed', 'trick', 'told', 'instruct', 'tri', 'look', 'could', 'wake', 'begin', 'took', 'rest', 'live', 'travel', 'love', 'spous']\n"
     ]
    }
   ],
   "source": [
    "print(unique_verbStems_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10.2 continued - make list into set\n",
    "verbStem_set = set(unique_verbStems_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'work', 'base', 'employ', 'need', 'achiev', 'took', 'detract', 'attract', 'tri', 'trust', 'catch', 'ground', 'struggl', 'describ', 'stretch', 'begin', 'listen', 'get', 'tell', 'feel', 'background', 'want', 'draw', 'know', 'see', 'spoke', 'live', 'look', 'emerg', 'disciplin', 'guess', 'check', 're', 'captur', 'spous', 'went', 'evolv', 'trick', 'told', 'wake', 'love', 'imagin', 'rest', 'say', 'exceed', 'would', 'felt', 'may', 'm', 'could', 'express', 'instruct', 'grow', 'think', 'happen', 'parent', 'travel', 've', 'keep', 'push', 'set', 'make', 'shift', 'partner', 's', 'ca', 'take', 'includ'}\n"
     ]
    }
   ],
   "source": [
    "print(verbStem_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus_nouns passed through this step without error so all_verbStems needs to be replaced by something which has the same form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['people time change transformation metamorphosis call future right past self self versions people journey kinds years side practice pharmaceutical side pharmaceutical companies work technology technology startup companies', 'strategies talk minute change changes zone barriers', 'write confidence', 'experts motivation tips ways number friends family change challenge admit regrets shame changes future days areas know husband patience ones areas child school dimensions kids kinds things brain', 'supporters detractors thing recognize today sense direction growth goals wellness background behavior change experts motivation tips ways number friends family change challenge admit regrets shame changes make', 'form school paper form dictation fact part app sales pitch way moment paper voice memos tools stream consciousness thoughts topic thing thing challenging thing thing reflect crystallize position hold beliefs shift career work home', 'comfort zone boundaries steps goals boundaries things placebo studies people year men weeks 20s mirror moment course study blood pressure heart rates markers', 'globe place water forest nature ones children friends scenery']\n"
     ]
    }
   ],
   "source": [
    "print(corpus_nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_verbStems = [\" \".join(lst) for lst in all_verbStems]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['say see imagin want listen think know went disciplin know work partner ve shift emerg base', 'employ know re guess', 'know take catch happen s', 'went ground check spoke trust struggl feel know felt need make felt would evolv know m work includ tell work know get draw know describ', 'think may detract make know ca grow need know know background went ground check spoke trust struggl feel know felt need', 's attract m felt express work captur may want need need think set parent set', 'keep stretch feel achiev push know exceed say take keep push trick told tell work instruct tri look tri could went wake begin took rest', 've live travel need s know know love could know spous']\n"
     ]
    }
   ],
   "source": [
    "print(corpus_verbStems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10.3 manual Term Frequeny calc\n",
    "\n",
    "n_docs_verbStems = len(all_verbStems)         #·Number of documents in the corpus\n",
    "n_words_verbStem_set = len(verbStem_set) #·Number of unique words in the\n",
    " \n",
    "df_tf_verbs = pd.DataFrame(np.zeros((n_docs_verbStems, n_words_verbStem_set)), columns=verbStem_set)\n",
    " \n",
    "# Compute Term Frequency (TF)\n",
    "for i in range(n_docs_verbStems):\n",
    "    words = corpus_verbStems[i].split(' ') # Words in the document\n",
    "    for w in words:\n",
    "        df_tf_verbs[w][i] = df_tf_verbs[w][i] + (1 / len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>work</th>\n",
       "      <th>base</th>\n",
       "      <th>employ</th>\n",
       "      <th>need</th>\n",
       "      <th>achiev</th>\n",
       "      <th>took</th>\n",
       "      <th>detract</th>\n",
       "      <th>attract</th>\n",
       "      <th>tri</th>\n",
       "      <th>trust</th>\n",
       "      <th>...</th>\n",
       "      <th>keep</th>\n",
       "      <th>push</th>\n",
       "      <th>set</th>\n",
       "      <th>make</th>\n",
       "      <th>shift</th>\n",
       "      <th>partner</th>\n",
       "      <th>s</th>\n",
       "      <th>ca</th>\n",
       "      <th>take</th>\n",
       "      <th>includ</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.0625</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0625</td>\n",
       "      <td>0.0625</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.080000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.095238</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.047619</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.047619</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.047619</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.047619</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.040000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 68 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       work    base  employ      need  achiev  took   detract   attract   tri  \\\n",
       "0  0.062500  0.0625    0.00  0.000000    0.00  0.00  0.000000  0.000000  0.00   \n",
       "1  0.000000  0.0000    0.25  0.000000    0.00  0.00  0.000000  0.000000  0.00   \n",
       "2  0.000000  0.0000    0.00  0.000000    0.00  0.00  0.000000  0.000000  0.00   \n",
       "3  0.080000  0.0000    0.00  0.040000    0.00  0.00  0.000000  0.000000  0.00   \n",
       "4  0.000000  0.0000    0.00  0.095238    0.00  0.00  0.047619  0.000000  0.00   \n",
       "5  0.066667  0.0000    0.00  0.133333    0.00  0.00  0.000000  0.066667  0.00   \n",
       "6  0.040000  0.0000    0.00  0.000000    0.04  0.04  0.000000  0.000000  0.08   \n",
       "7  0.000000  0.0000    0.00  0.090909    0.00  0.00  0.000000  0.000000  0.00   \n",
       "\n",
       "      trust  ...  keep  push       set      make   shift  partner         s  \\\n",
       "0  0.000000  ...  0.00  0.00  0.000000  0.000000  0.0625   0.0625  0.000000   \n",
       "1  0.000000  ...  0.00  0.00  0.000000  0.000000  0.0000   0.0000  0.000000   \n",
       "2  0.000000  ...  0.00  0.00  0.000000  0.000000  0.0000   0.0000  0.200000   \n",
       "3  0.040000  ...  0.00  0.00  0.000000  0.040000  0.0000   0.0000  0.000000   \n",
       "4  0.047619  ...  0.00  0.00  0.000000  0.047619  0.0000   0.0000  0.000000   \n",
       "5  0.000000  ...  0.00  0.00  0.133333  0.000000  0.0000   0.0000  0.066667   \n",
       "6  0.000000  ...  0.08  0.08  0.000000  0.000000  0.0000   0.0000  0.000000   \n",
       "7  0.000000  ...  0.00  0.00  0.000000  0.000000  0.0000   0.0000  0.090909   \n",
       "\n",
       "         ca  take  includ  \n",
       "0  0.000000  0.00    0.00  \n",
       "1  0.000000  0.00    0.00  \n",
       "2  0.000000  0.20    0.00  \n",
       "3  0.000000  0.00    0.04  \n",
       "4  0.047619  0.00    0.00  \n",
       "5  0.000000  0.00    0.00  \n",
       "6  0.000000  0.04    0.00  \n",
       "7  0.000000  0.00    0.00  \n",
       "\n",
       "[8 rows x 68 columns]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tf_verbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IDF of: \n",
      "           work: 0.3010299956639812\n",
      "           base: 0.9030899869919435\n",
      "         employ: 0.9030899869919435\n",
      "           need: 0.3010299956639812\n",
      "         achiev: 0.9030899869919435\n",
      "           took: 0.9030899869919435\n",
      "        detract: 0.9030899869919435\n",
      "        attract: 0.9030899869919435\n",
      "            tri: 0.9030899869919435\n",
      "          trust: 0.6020599913279624\n",
      "          catch: 0.9030899869919435\n",
      "         ground: 0.6020599913279624\n",
      "        struggl: 0.6020599913279624\n",
      "        describ: 0.9030899869919435\n",
      "        stretch: 0.9030899869919435\n",
      "          begin: 0.9030899869919435\n",
      "         listen: 0.9030899869919435\n",
      "            get: 0.9030899869919435\n",
      "           tell: 0.6020599913279624\n",
      "           feel: 0.4259687322722811\n",
      "     background: 0.9030899869919435\n",
      "           want: 0.6020599913279624\n",
      "           draw: 0.9030899869919435\n",
      "           know: 0.05799194697768673\n",
      "            see: 0.9030899869919435\n",
      "          spoke: 0.6020599913279624\n",
      "           live: 0.9030899869919435\n",
      "           look: 0.9030899869919435\n",
      "          emerg: 0.9030899869919435\n",
      "      disciplin: 0.9030899869919435\n",
      "          guess: 0.9030899869919435\n",
      "          check: 0.6020599913279624\n",
      "             re: 0.9030899869919435\n",
      "         captur: 0.9030899869919435\n",
      "          spous: 0.9030899869919435\n",
      "           went: 0.3010299956639812\n",
      "          evolv: 0.9030899869919435\n",
      "          trick: 0.9030899869919435\n",
      "           told: 0.9030899869919435\n",
      "           wake: 0.9030899869919435\n",
      "           love: 0.9030899869919435\n",
      "         imagin: 0.9030899869919435\n",
      "           rest: 0.9030899869919435\n",
      "            say: 0.6020599913279624\n",
      "         exceed: 0.9030899869919435\n",
      "          would: 0.9030899869919435\n",
      "           felt: 0.4259687322722811\n",
      "            may: 0.6020599913279624\n",
      "              m: 0.6020599913279624\n",
      "          could: 0.6020599913279624\n",
      "        express: 0.9030899869919435\n",
      "       instruct: 0.9030899869919435\n",
      "           grow: 0.9030899869919435\n",
      "          think: 0.4259687322722811\n",
      "         happen: 0.9030899869919435\n",
      "         parent: 0.9030899869919435\n",
      "         travel: 0.9030899869919435\n",
      "             ve: 0.6020599913279624\n",
      "           keep: 0.9030899869919435\n",
      "           push: 0.9030899869919435\n",
      "            set: 0.9030899869919435\n",
      "           make: 0.6020599913279624\n",
      "          shift: 0.9030899869919435\n",
      "        partner: 0.9030899869919435\n",
      "              s: 0.4259687322722811\n",
      "             ca: 0.9030899869919435\n",
      "           take: 0.6020599913279624\n",
      "         includ: 0.9030899869919435\n"
     ]
    }
   ],
   "source": [
    "# 10.4 manual inverse document frequency (IDF) calc\n",
    "\n",
    "print(\"IDF of: \")\n",
    " \n",
    "idf_verbs = {}\n",
    " \n",
    "for w in verbStem_set:\n",
    "    k = 0    # number of documents in the corpus that contain this word\n",
    "     \n",
    "    for i in range(n_docs_verbStems):\n",
    "        if w in corpus_verbStems[i].split():\n",
    "            k += 1\n",
    "             \n",
    "    idf_verbs[w] =  np.log10(n_docs_verbStems / k)\n",
    "     \n",
    "    print(f'{w:>15}: {idf_verbs[w]:>10}' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10.4 continued\n",
    "df_tf_verbs_idf = df_tf_verbs.copy()\n",
    " \n",
    "for w in verbStem_set:\n",
    "    for i in range(n_docs_verbStems):\n",
    "        df_tf_verbs_idf[w][i] = df_tf_verbs[w][i] * idf_verbs[w]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>work</th>\n",
       "      <th>base</th>\n",
       "      <th>employ</th>\n",
       "      <th>need</th>\n",
       "      <th>achiev</th>\n",
       "      <th>took</th>\n",
       "      <th>detract</th>\n",
       "      <th>attract</th>\n",
       "      <th>tri</th>\n",
       "      <th>trust</th>\n",
       "      <th>...</th>\n",
       "      <th>keep</th>\n",
       "      <th>push</th>\n",
       "      <th>set</th>\n",
       "      <th>make</th>\n",
       "      <th>shift</th>\n",
       "      <th>partner</th>\n",
       "      <th>s</th>\n",
       "      <th>ca</th>\n",
       "      <th>take</th>\n",
       "      <th>includ</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.018814</td>\n",
       "      <td>0.056443</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.056443</td>\n",
       "      <td>0.056443</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.225772</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.085194</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.120412</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.024082</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012041</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.024082</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.024082</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.036124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.028670</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.043004</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.028670</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.028670</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.043004</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.020069</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.040137</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.060206</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.120412</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.028398</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.012041</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.036124</td>\n",
       "      <td>0.036124</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.072247</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.072247</td>\n",
       "      <td>0.072247</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.024082</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.027366</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.038724</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 68 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       work      base    employ      need    achiev      took   detract  \\\n",
       "0  0.018814  0.056443  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "1  0.000000  0.000000  0.225772  0.000000  0.000000  0.000000  0.000000   \n",
       "2  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "3  0.024082  0.000000  0.000000  0.012041  0.000000  0.000000  0.000000   \n",
       "4  0.000000  0.000000  0.000000  0.028670  0.000000  0.000000  0.043004   \n",
       "5  0.020069  0.000000  0.000000  0.040137  0.000000  0.000000  0.000000   \n",
       "6  0.012041  0.000000  0.000000  0.000000  0.036124  0.036124  0.000000   \n",
       "7  0.000000  0.000000  0.000000  0.027366  0.000000  0.000000  0.000000   \n",
       "\n",
       "    attract       tri     trust  ...      keep      push       set      make  \\\n",
       "0  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
       "1  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
       "2  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
       "3  0.000000  0.000000  0.024082  ...  0.000000  0.000000  0.000000  0.024082   \n",
       "4  0.000000  0.000000  0.028670  ...  0.000000  0.000000  0.000000  0.028670   \n",
       "5  0.060206  0.000000  0.000000  ...  0.000000  0.000000  0.120412  0.000000   \n",
       "6  0.000000  0.072247  0.000000  ...  0.072247  0.072247  0.000000  0.000000   \n",
       "7  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "      shift   partner         s        ca      take    includ  \n",
       "0  0.056443  0.056443  0.000000  0.000000  0.000000  0.000000  \n",
       "1  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "2  0.000000  0.000000  0.085194  0.000000  0.120412  0.000000  \n",
       "3  0.000000  0.000000  0.000000  0.000000  0.000000  0.036124  \n",
       "4  0.000000  0.000000  0.000000  0.043004  0.000000  0.000000  \n",
       "5  0.000000  0.000000  0.028398  0.000000  0.000000  0.000000  \n",
       "6  0.000000  0.000000  0.000000  0.000000  0.024082  0.000000  \n",
       "7  0.000000  0.000000  0.038724  0.000000  0.000000  0.000000  \n",
       "\n",
       "[8 rows x 68 columns]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tf_verbs_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10.5 apply sklearn TfidfVectorizer\n",
    "tr_idf_verbs_model  = TfidfVectorizer()\n",
    "tf_idf_verbs_vector = tr_idf_verbs_model.fit_transform(corpus_verbStems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10.5 continued\n",
    "verbStem_set = tr_idf_verbs_model.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10.5 continued\n",
    "tf_idf_verbs_array = tf_idf_verbs_vector.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10.5 continued\n",
    "df_tf_verbs_idf = pd.DataFrame(tf_idf_verbs_array, columns = verbStem_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>achiev</th>\n",
       "      <th>attract</th>\n",
       "      <th>background</th>\n",
       "      <th>base</th>\n",
       "      <th>begin</th>\n",
       "      <th>ca</th>\n",
       "      <th>captur</th>\n",
       "      <th>catch</th>\n",
       "      <th>check</th>\n",
       "      <th>could</th>\n",
       "      <th>...</th>\n",
       "      <th>travel</th>\n",
       "      <th>tri</th>\n",
       "      <th>trick</th>\n",
       "      <th>trust</th>\n",
       "      <th>ve</th>\n",
       "      <th>wake</th>\n",
       "      <th>want</th>\n",
       "      <th>went</th>\n",
       "      <th>work</th>\n",
       "      <th>would</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.285933</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.239634</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.239634</td>\n",
       "      <td>0.181304</td>\n",
       "      <td>0.181304</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.587055</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.191603</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.191603</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.144965</td>\n",
       "      <td>0.289929</td>\n",
       "      <td>0.228622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.252505</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.252505</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.211619</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.211619</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.160109</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.283284</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.283284</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.237414</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.179625</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.191264</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.191264</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.160294</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.382528</td>\n",
       "      <td>0.191264</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.191264</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.121277</td>\n",
       "      <td>0.121277</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.304000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.362735</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.304000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 66 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     achiev   attract  background      base     begin        ca    captur  \\\n",
       "0  0.000000  0.000000    0.000000  0.285933  0.000000  0.000000  0.000000   \n",
       "1  0.000000  0.000000    0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "2  0.000000  0.000000    0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "3  0.000000  0.000000    0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "4  0.000000  0.000000    0.252505  0.000000  0.000000  0.252505  0.000000   \n",
       "5  0.000000  0.283284    0.000000  0.000000  0.000000  0.000000  0.283284   \n",
       "6  0.191264  0.000000    0.000000  0.000000  0.191264  0.000000  0.000000   \n",
       "7  0.000000  0.000000    0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "      catch     check     could  ...    travel       tri     trick     trust  \\\n",
       "0  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
       "1  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
       "2  0.587055  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
       "3  0.000000  0.191603  0.000000  ...  0.000000  0.000000  0.000000  0.191603   \n",
       "4  0.000000  0.211619  0.000000  ...  0.000000  0.000000  0.000000  0.211619   \n",
       "5  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
       "6  0.000000  0.000000  0.160294  ...  0.000000  0.382528  0.191264  0.000000   \n",
       "7  0.000000  0.000000  0.304000  ...  0.362735  0.000000  0.000000  0.000000   \n",
       "\n",
       "         ve      wake      want      went      work     would  \n",
       "0  0.239634  0.000000  0.239634  0.181304  0.181304  0.000000  \n",
       "1  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "2  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "3  0.000000  0.000000  0.000000  0.144965  0.289929  0.228622  \n",
       "4  0.000000  0.000000  0.000000  0.160109  0.000000  0.000000  \n",
       "5  0.000000  0.000000  0.237414  0.000000  0.179625  0.000000  \n",
       "6  0.000000  0.191264  0.000000  0.121277  0.121277  0.000000  \n",
       "7  0.304000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "\n",
       "[8 rows x 66 columns]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tf_verbs_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#10.5.1 Final shape\n",
    "cols = df_tf_verbs_idf.columns\n",
    "bt = df_tf_verbs_idf.apply(lambda x: x > 0)\n",
    "final_shape_verbs = [bt.apply(lambda x: list(cols[x.values]), axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0    [base, disciplin, emerg, imagin, know, listen,...\n",
      "1                            [employ, guess, know, re]\n",
      "2                          [catch, happen, know, take]\n",
      "3    [check, describ, draw, evolv, feel, felt, get,...\n",
      "4    [background, ca, check, detract, feel, felt, g...\n",
      "5    [attract, captur, express, felt, may, need, pa...\n",
      "6    [achiev, begin, could, exceed, feel, instruct,...\n",
      "7    [could, know, live, love, need, spous, travel,...\n",
      "dtype: object]\n"
     ]
    }
   ],
   "source": [
    "print(final_shape_verbs) #might be better as dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11.0 Adjs and Advs\n",
    "    # make a corpus from all 8 files\n",
    "    # make a word-set (vocab of unique words) of the corpus\n",
    "    # manual Term Frequeny calc\n",
    "    # manual inverse document frequency (IDF) calc\n",
    "    # apply sklearn TfidfVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11.1 make a corpus from all 8 files\n",
    "\n",
    "# https://www.programiz.com/python-programming/examples/flatten-nested-list\n",
    "# step 7\n",
    "# how to put all_nouns into right format and pass through? It needs to be a list of strings not a list of lists. Inner \n",
    "# list must be joined.\n",
    "\n",
    "# 11.1.2\n",
    "    \n",
    "# all_depunc is feed doc\n",
    "# https://www.programiz.com/python-programming/examples/flatten-nested-list\n",
    "corpus_adjs = [\" \".join(lst) for lst in all_adjs]\n",
    "corpus_advs = [\" \".join(lst) for lst in all_advs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hard potential different present finished several different clinical therapeutic smaller', 'simple ill multidimensional fundamental', '', 'authentic small incredible incredible vulnerable vulnerable overwhelmed scared early many different', 'important much important own mental authentic small incredible incredible vulnerable vulnerable overwhelmed', 'old evermore great happy sad nervous unquote wrong', 'small small great clinical old imagine much much biological', 'little much simple vast untouched low key quiet beautiful']\n"
     ]
    }
   ],
   "source": [
    "print(corpus_adjs) #print(corpus_adjs[2]) is empty. Let's foce it for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(corpus_adjs[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_adjs[2]=\"replacementword\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hard potential different present finished several different clinical therapeutic smaller', 'simple ill multidimensional fundamental', 'replacementword', 'authentic small incredible incredible vulnerable vulnerable overwhelmed scared early many different', 'important much important own mental authentic small incredible incredible vulnerable vulnerable overwhelmed', 'old evermore great happy sad nervous unquote wrong', 'small small great clinical old imagine much much biological', 'little much simple vast untouched low key quiet beautiful']\n"
     ]
    }
   ],
   "source": [
    "print(corpus_adjs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11.2 create a word set/vocab for the verbs' corpus\n",
    "adj_words_set = set()\n",
    "adv_words_set = set()\n",
    " \n",
    "for doc in corpus_adjs:\n",
    "    words_adjs = doc.split(' ')\n",
    "    adj_words_set = adj_words_set.union(set(words_adjs))\n",
    "    \n",
    "for doc in corpus_advs:\n",
    "    words_advs = doc.split(' ')\n",
    "    adv_words_set = adv_words_set.union(set(words_advs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "#11.3 manual Term Frequeny calc\n",
    "            #11.3.1 adjectives\n",
    "\n",
    "n_docs_adjs = len(corpus_adjs)         #·Number of documents in the corpus\n",
    "n_words_set_adjs = len(adj_words_set) #·Number of unique words in the\n",
    " \n",
    "df_tf_adjs = pd.DataFrame(np.zeros((n_docs_adjs, n_words_set_adjs)), columns=adj_words_set)\n",
    " \n",
    "# Compute Term Frequency (TF)\n",
    "for i in range(n_docs_adjs):\n",
    "    words = corpus_adjs[i].split(' ') # Words in the document\n",
    "    for w in words:\n",
    "        df_tf_adjs[w][i] = df_tf_adjs[w][i] + (1 / len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "#11.3.2 adverbs\n",
    "\n",
    "n_docs_advs = len(corpus_advs)         #·Number of documents in the corpus\n",
    "n_words_set_advs = len(adv_words_set) #·Number of unique words in the\n",
    " \n",
    "df_tf_advs = pd.DataFrame(np.zeros((n_docs_advs, n_words_set_advs)), columns=adv_words_set)\n",
    " \n",
    "# Compute Term Frequency (TF)\n",
    "for i in range(n_docs_advs):\n",
    "    words = corpus_advs[i].split(' ') # Words in the document\n",
    "    for w in words:\n",
    "        df_tf_advs[w][i] = df_tf_advs[w][i] + (1 / len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IDF of: \n",
      "     incredible: 0.6020599913279624\n",
      "          early: 0.9030899869919435\n",
      "    overwhelmed: 0.6020599913279624\n",
      "           hard: 0.9030899869919435\n",
      "multidimensional: 0.9030899869919435\n",
      "           much: 0.4259687322722811\n",
      "        unquote: 0.9030899869919435\n",
      "            low: 0.9030899869919435\n",
      "            old: 0.6020599913279624\n",
      "         simple: 0.6020599913279624\n",
      "       clinical: 0.6020599913279624\n",
      "      authentic: 0.6020599913279624\n",
      "        imagine: 0.9030899869919435\n",
      "     biological: 0.9030899869919435\n",
      "     vulnerable: 0.6020599913279624\n",
      "            key: 0.9030899869919435\n",
      "          quiet: 0.9030899869919435\n",
      "      important: 0.9030899869919435\n",
      "    therapeutic: 0.9030899869919435\n",
      "      potential: 0.9030899869919435\n",
      "replacementword: 0.9030899869919435\n",
      "            own: 0.9030899869919435\n",
      "           vast: 0.9030899869919435\n",
      "      beautiful: 0.9030899869919435\n",
      "         little: 0.9030899869919435\n",
      "          wrong: 0.9030899869919435\n",
      "        present: 0.9030899869919435\n",
      "      different: 0.6020599913279624\n",
      "          small: 0.4259687322722811\n",
      "        smaller: 0.9030899869919435\n",
      "       finished: 0.9030899869919435\n",
      "    fundamental: 0.9030899869919435\n",
      "         scared: 0.9030899869919435\n",
      "          great: 0.6020599913279624\n",
      "            ill: 0.9030899869919435\n",
      "           many: 0.9030899869919435\n",
      "          happy: 0.9030899869919435\n",
      "      untouched: 0.9030899869919435\n",
      "         mental: 0.9030899869919435\n",
      "            sad: 0.9030899869919435\n",
      "       evermore: 0.9030899869919435\n",
      "        nervous: 0.9030899869919435\n",
      "        several: 0.9030899869919435\n"
     ]
    }
   ],
   "source": [
    "# 11.4 manual inverse document frequency (IDF) calc\n",
    "    #11.4.1 adjectives\n",
    "print(\"IDF of: \")\n",
    " \n",
    "idf_adjs = {}\n",
    " \n",
    "for w in adj_words_set:\n",
    "    k = 0    # number of documents in the corpus that contain this word\n",
    "     \n",
    "    for i in range(n_docs_adjs):\n",
    "        if w in corpus_adjs[i].split():\n",
    "            k += 1\n",
    "             \n",
    "    idf_adjs[w] =  np.log10(n_docs_adjs / k)\n",
    "     \n",
    "    print(f'{w:>15}: {idf_adjs[w]:>10}' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11.4 continued\n",
    "       # 11.4.1 Adjectives\n",
    "df_tf_adjs_idf = df_tf_adjs.copy()\n",
    " \n",
    "for w in adj_words_set:\n",
    "    for i in range(n_docs_adjs):\n",
    "        df_tf_adjs_idf[w][i] = df_tf_adjs[w][i] * idf_adjs[w]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>incredible</th>\n",
       "      <th>early</th>\n",
       "      <th>overwhelmed</th>\n",
       "      <th>hard</th>\n",
       "      <th>multidimensional</th>\n",
       "      <th>much</th>\n",
       "      <th>unquote</th>\n",
       "      <th>low</th>\n",
       "      <th>old</th>\n",
       "      <th>simple</th>\n",
       "      <th>...</th>\n",
       "      <th>great</th>\n",
       "      <th>ill</th>\n",
       "      <th>many</th>\n",
       "      <th>happy</th>\n",
       "      <th>untouched</th>\n",
       "      <th>mental</th>\n",
       "      <th>sad</th>\n",
       "      <th>evermore</th>\n",
       "      <th>nervous</th>\n",
       "      <th>several</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.090309</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.090309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.225772</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.150515</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.225772</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.109465</td>\n",
       "      <td>0.082099</td>\n",
       "      <td>0.054733</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.082099</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.100343</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.050172</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.035497</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.075257</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.112886</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.075257</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.075257</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.112886</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.112886</td>\n",
       "      <td>0.112886</td>\n",
       "      <td>0.112886</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.094660</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.066896</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.066896</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.047330</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.100343</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.066896</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.100343</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 43 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   incredible     early  overwhelmed      hard  multidimensional      much  \\\n",
       "0    0.000000  0.000000     0.000000  0.090309          0.000000  0.000000   \n",
       "1    0.000000  0.000000     0.000000  0.000000          0.225772  0.000000   \n",
       "2    0.000000  0.000000     0.000000  0.000000          0.000000  0.000000   \n",
       "3    0.109465  0.082099     0.054733  0.000000          0.000000  0.000000   \n",
       "4    0.100343  0.000000     0.050172  0.000000          0.000000  0.035497   \n",
       "5    0.000000  0.000000     0.000000  0.000000          0.000000  0.000000   \n",
       "6    0.000000  0.000000     0.000000  0.000000          0.000000  0.094660   \n",
       "7    0.000000  0.000000     0.000000  0.000000          0.000000  0.047330   \n",
       "\n",
       "    unquote       low       old    simple  ...     great       ill      many  \\\n",
       "0  0.000000  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "1  0.000000  0.000000  0.000000  0.150515  ...  0.000000  0.225772  0.000000   \n",
       "2  0.000000  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "3  0.000000  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.082099   \n",
       "4  0.000000  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "5  0.112886  0.000000  0.075257  0.000000  ...  0.075257  0.000000  0.000000   \n",
       "6  0.000000  0.000000  0.066896  0.000000  ...  0.066896  0.000000  0.000000   \n",
       "7  0.000000  0.100343  0.000000  0.066896  ...  0.000000  0.000000  0.000000   \n",
       "\n",
       "      happy  untouched    mental       sad  evermore   nervous   several  \n",
       "0  0.000000   0.000000  0.000000  0.000000  0.000000  0.000000  0.090309  \n",
       "1  0.000000   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "2  0.000000   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "3  0.000000   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "4  0.000000   0.000000  0.075257  0.000000  0.000000  0.000000  0.000000  \n",
       "5  0.112886   0.000000  0.000000  0.112886  0.112886  0.112886  0.000000  \n",
       "6  0.000000   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "7  0.000000   0.100343  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "\n",
       "[8 rows x 43 columns]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tf_adjs_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IDF of: \n",
      "    essentially: 0.9030899869919435\n",
      "           sort: 0.9030899869919435\n",
      "      knowingly: 0.9030899869919435\n",
      "    effectively: 0.9030899869919435\n",
      "        further: 0.9030899869919435\n",
      "        quickly: 0.9030899869919435\n",
      "         really: 0.3010299956639812\n",
      "           well: 0.6020599913279624\n",
      "         almost: 0.9030899869919435\n",
      "          again: 0.9030899869919435\n",
      "       mentally: 0.9030899869919435\n",
      "        clearly: 0.9030899869919435\n",
      "     definitely: 0.9030899869919435\n",
      "       slightly: 0.9030899869919435\n",
      "     absolutely: 0.9030899869919435\n",
      "       somewhat: 0.9030899869919435\n",
      "          alone: 0.9030899869919435\n",
      "           even: 0.9030899869919435\n",
      "          still: 0.2041199826559248\n",
      "      privately: 0.9030899869919435\n",
      "        however: 0.9030899869919435\n",
      "            yet: 0.9030899869919435\n",
      "         nearby: 0.9030899869919435\n",
      "           kind: 0.4259687322722811\n",
      "             so: 0.9030899869919435\n",
      "          there: 0.9030899869919435\n",
      "          right: 0.6020599913279624\n",
      "      overnight: 0.9030899869919435\n",
      "           best: 0.9030899869919435\n",
      "      fortunate: 0.9030899869919435\n",
      "           real: 0.9030899869919435\n",
      "         pretty: 0.9030899869919435\n"
     ]
    }
   ],
   "source": [
    "#11.4.2 adverbs\n",
    "\n",
    "print(\"IDF of: \")\n",
    " \n",
    "idf_advs = {}\n",
    " \n",
    "for w in adv_words_set:\n",
    "    k = 0    # number of documents in the corpus that contain this word\n",
    "     \n",
    "    for i in range(n_docs_advs):\n",
    "        if w in corpus_advs[i].split():\n",
    "            k += 1\n",
    "             \n",
    "    idf_advs[w] =  np.log10(n_docs_advs / k)\n",
    "     \n",
    "    print(f'{w:>15}: {idf_advs[w]:>10}' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11.4.2 Adverbs\n",
    "df_tf_advs_idf = df_tf_advs.copy()\n",
    " \n",
    "for w in adv_words_set:\n",
    "    for i in range(n_docs_advs):\n",
    "        df_tf_advs_idf[w][i] = df_tf_advs[w][i] * idf_advs[w]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essentially</th>\n",
       "      <th>sort</th>\n",
       "      <th>knowingly</th>\n",
       "      <th>effectively</th>\n",
       "      <th>further</th>\n",
       "      <th>quickly</th>\n",
       "      <th>really</th>\n",
       "      <th>well</th>\n",
       "      <th>almost</th>\n",
       "      <th>again</th>\n",
       "      <th>...</th>\n",
       "      <th>nearby</th>\n",
       "      <th>kind</th>\n",
       "      <th>so</th>\n",
       "      <th>there</th>\n",
       "      <th>right</th>\n",
       "      <th>overnight</th>\n",
       "      <th>best</th>\n",
       "      <th>fortunate</th>\n",
       "      <th>real</th>\n",
       "      <th>pretty</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.043004</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.086009</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.129013</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.425969</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.451545</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.100343</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.066896</td>\n",
       "      <td>0.100343</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.133791</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.112886</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.075257</td>\n",
       "      <td>0.075257</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.112886</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.090309</td>\n",
       "      <td>0.060206</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.090309</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.085194</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.090309</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.090309</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.129013</td>\n",
       "      <td>0.129013</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.129013</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.043004</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.121705</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.180618</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.180618</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.180618</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   essentially      sort  knowingly  effectively   further   quickly  \\\n",
       "0     0.000000  0.000000   0.000000     0.000000  0.000000  0.000000   \n",
       "1     0.000000  0.000000   0.000000     0.000000  0.000000  0.000000   \n",
       "2     0.000000  0.000000   0.000000     0.000000  0.000000  0.000000   \n",
       "3     0.100343  0.000000   0.000000     0.000000  0.000000  0.000000   \n",
       "4     0.000000  0.000000   0.000000     0.112886  0.000000  0.000000   \n",
       "5     0.000000  0.000000   0.000000     0.000000  0.000000  0.090309   \n",
       "6     0.000000  0.129013   0.129013     0.000000  0.129013  0.000000   \n",
       "7     0.000000  0.000000   0.000000     0.000000  0.000000  0.000000   \n",
       "\n",
       "     really      well    almost     again  ...    nearby      kind        so  \\\n",
       "0  0.043004  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "1  0.000000  0.000000  0.000000  0.000000  ...  0.000000  0.425969  0.000000   \n",
       "2  0.000000  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "3  0.000000  0.066896  0.100343  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "4  0.075257  0.075257  0.000000  0.000000  ...  0.000000  0.000000  0.112886   \n",
       "5  0.060206  0.000000  0.000000  0.090309  ...  0.000000  0.085194  0.000000   \n",
       "6  0.043004  0.000000  0.000000  0.000000  ...  0.000000  0.121705  0.000000   \n",
       "7  0.000000  0.000000  0.000000  0.000000  ...  0.180618  0.000000  0.000000   \n",
       "\n",
       "      there     right  overnight      best  fortunate      real    pretty  \n",
       "0  0.000000  0.086009   0.000000  0.000000   0.000000  0.129013  0.000000  \n",
       "1  0.000000  0.000000   0.000000  0.000000   0.000000  0.000000  0.000000  \n",
       "2  0.000000  0.000000   0.451545  0.000000   0.000000  0.000000  0.000000  \n",
       "3  0.000000  0.133791   0.000000  0.000000   0.000000  0.000000  0.000000  \n",
       "4  0.000000  0.000000   0.000000  0.000000   0.000000  0.000000  0.000000  \n",
       "5  0.090309  0.000000   0.000000  0.090309   0.000000  0.000000  0.000000  \n",
       "6  0.000000  0.000000   0.000000  0.000000   0.000000  0.000000  0.000000  \n",
       "7  0.000000  0.000000   0.000000  0.000000   0.180618  0.000000  0.180618  \n",
       "\n",
       "[8 rows x 32 columns]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tf_advs_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11.5 apply sklearn TfidfVectorizer\n",
    "        # 11.5.1 adjectives\n",
    "tr_idf_adjs_model  = TfidfVectorizer()\n",
    "tf_idf_adjs_vector = tr_idf_adjs_model.fit_transform(corpus_adjs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11.5.1 continued\n",
    "adj_words_set = tr_idf_adjs_model.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11.5.1 continued\n",
    "tf_idf_adjs_array = tf_idf_adjs_vector.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11.5.1 continued\n",
    "df_tf_adjs_idf = pd.DataFrame(tf_idf_adjs_array, columns = adj_words_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>authentic</th>\n",
       "      <th>beautiful</th>\n",
       "      <th>biological</th>\n",
       "      <th>clinical</th>\n",
       "      <th>different</th>\n",
       "      <th>early</th>\n",
       "      <th>evermore</th>\n",
       "      <th>finished</th>\n",
       "      <th>fundamental</th>\n",
       "      <th>great</th>\n",
       "      <th>...</th>\n",
       "      <th>several</th>\n",
       "      <th>simple</th>\n",
       "      <th>small</th>\n",
       "      <th>smaller</th>\n",
       "      <th>therapeutic</th>\n",
       "      <th>unquote</th>\n",
       "      <th>untouched</th>\n",
       "      <th>vast</th>\n",
       "      <th>vulnerable</th>\n",
       "      <th>wrong</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.258490</td>\n",
       "      <td>0.516981</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.308432</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.308432</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.308432</td>\n",
       "      <td>0.308432</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.519708</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.435556</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.249876</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.249876</td>\n",
       "      <td>0.298154</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.215623</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.499753</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.223430</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.192802</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.446860</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.367489</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.307985</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.367489</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.367489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.34729</td>\n",
       "      <td>0.291056</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.291056</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.502314</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.348676</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.292218</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.348676</td>\n",
       "      <td>0.348676</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 43 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   authentic  beautiful  biological  clinical  different     early  evermore  \\\n",
       "0   0.000000   0.000000     0.00000  0.258490   0.516981  0.000000  0.000000   \n",
       "1   0.000000   0.000000     0.00000  0.000000   0.000000  0.000000  0.000000   \n",
       "2   0.000000   0.000000     0.00000  0.000000   0.000000  0.000000  0.000000   \n",
       "3   0.249876   0.000000     0.00000  0.000000   0.249876  0.298154  0.000000   \n",
       "4   0.223430   0.000000     0.00000  0.000000   0.000000  0.000000  0.000000   \n",
       "5   0.000000   0.000000     0.00000  0.000000   0.000000  0.000000  0.367489   \n",
       "6   0.000000   0.000000     0.34729  0.291056   0.000000  0.000000  0.000000   \n",
       "7   0.000000   0.348676     0.00000  0.000000   0.000000  0.000000  0.000000   \n",
       "\n",
       "   finished  fundamental     great  ...   several    simple     small  \\\n",
       "0  0.308432     0.000000  0.000000  ...  0.308432  0.000000  0.000000   \n",
       "1  0.000000     0.519708  0.000000  ...  0.000000  0.435556  0.000000   \n",
       "2  0.000000     0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "3  0.000000     0.000000  0.000000  ...  0.000000  0.000000  0.215623   \n",
       "4  0.000000     0.000000  0.000000  ...  0.000000  0.000000  0.192802   \n",
       "5  0.000000     0.000000  0.307985  ...  0.000000  0.000000  0.000000   \n",
       "6  0.000000     0.000000  0.291056  ...  0.000000  0.000000  0.502314   \n",
       "7  0.000000     0.000000  0.000000  ...  0.000000  0.292218  0.000000   \n",
       "\n",
       "    smaller  therapeutic   unquote  untouched      vast  vulnerable     wrong  \n",
       "0  0.308432     0.308432  0.000000   0.000000  0.000000    0.000000  0.000000  \n",
       "1  0.000000     0.000000  0.000000   0.000000  0.000000    0.000000  0.000000  \n",
       "2  0.000000     0.000000  0.000000   0.000000  0.000000    0.000000  0.000000  \n",
       "3  0.000000     0.000000  0.000000   0.000000  0.000000    0.499753  0.000000  \n",
       "4  0.000000     0.000000  0.000000   0.000000  0.000000    0.446860  0.000000  \n",
       "5  0.000000     0.000000  0.367489   0.000000  0.000000    0.000000  0.367489  \n",
       "6  0.000000     0.000000  0.000000   0.000000  0.000000    0.000000  0.000000  \n",
       "7  0.000000     0.000000  0.000000   0.348676  0.348676    0.000000  0.000000  \n",
       "\n",
       "[8 rows x 43 columns]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tf_adjs_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "#11.5.1.1 Final shape\n",
    "cols = df_tf_adjs_idf.columns\n",
    "bt = df_tf_adjs_idf.apply(lambda x: x > 0)\n",
    "final_shape_adjs = [bt.apply(lambda x: list(cols[x.values]), axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0    [clinical, different, finished, hard, potentia...\n",
      "1         [fundamental, ill, multidimensional, simple]\n",
      "2                                    [replacementword]\n",
      "3    [authentic, different, early, incredible, many...\n",
      "4    [authentic, important, incredible, mental, muc...\n",
      "5    [evermore, great, happy, nervous, old, sad, un...\n",
      "6    [biological, clinical, great, imagine, much, o...\n",
      "7    [beautiful, key, little, low, much, quiet, sim...\n",
      "dtype: object]\n"
     ]
    }
   ],
   "source": [
    "print(final_shape_adjs) #might be better as dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11.5.2 adverbs\n",
    "tr_idf_advs_model  = TfidfVectorizer()\n",
    "tf_idf_advs_vector = tr_idf_advs_model.fit_transform(corpus_advs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11.5.2 continued\n",
    "adv_words_set = tr_idf_advs_model.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11.5.2 continued\n",
    "tf_idf_advs_array = tf_idf_advs_vector.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11.5.2 continued\n",
    "df_tf_advs_idf = pd.DataFrame(tf_idf_advs_array, columns = adv_words_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>absolutely</th>\n",
       "      <th>again</th>\n",
       "      <th>almost</th>\n",
       "      <th>alone</th>\n",
       "      <th>best</th>\n",
       "      <th>clearly</th>\n",
       "      <th>definitely</th>\n",
       "      <th>effectively</th>\n",
       "      <th>essentially</th>\n",
       "      <th>even</th>\n",
       "      <th>...</th>\n",
       "      <th>really</th>\n",
       "      <th>right</th>\n",
       "      <th>slightly</th>\n",
       "      <th>so</th>\n",
       "      <th>somewhat</th>\n",
       "      <th>sort</th>\n",
       "      <th>still</th>\n",
       "      <th>there</th>\n",
       "      <th>well</th>\n",
       "      <th>yet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.429558</td>\n",
       "      <td>0.429558</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.272375</td>\n",
       "      <td>0.360003</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.241098</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.337638</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.337638</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.565934</td>\n",
       "      <td>0.337638</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.337638</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.379013</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.282967</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.388496</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.388496</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.492675</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.388496</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.218051</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.325590</td>\n",
       "      <td>0.388496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.321076</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.321076</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.321076</td>\n",
       "      <td>...</td>\n",
       "      <td>0.407176</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.321076</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.263081</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.414902</td>\n",
       "      <td>0.232872</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.481403</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.270197</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   absolutely     again    almost     alone      best   clearly  definitely  \\\n",
       "0    0.000000  0.000000  0.000000  0.000000  0.000000  0.429558    0.429558   \n",
       "1    0.000000  0.000000  0.000000  0.000000  0.000000  0.000000    0.000000   \n",
       "2    0.000000  0.000000  0.000000  0.000000  0.000000  0.000000    0.000000   \n",
       "3    0.000000  0.000000  0.337638  0.000000  0.000000  0.000000    0.000000   \n",
       "4    0.388496  0.000000  0.000000  0.000000  0.000000  0.000000    0.000000   \n",
       "5    0.000000  0.321076  0.000000  0.000000  0.321076  0.000000    0.000000   \n",
       "6    0.000000  0.000000  0.000000  0.000000  0.000000  0.000000    0.000000   \n",
       "7    0.000000  0.000000  0.000000  0.481403  0.000000  0.000000    0.000000   \n",
       "\n",
       "   effectively  essentially      even  ...    really     right  slightly  \\\n",
       "0     0.000000     0.000000  0.000000  ...  0.272375  0.360003  0.000000   \n",
       "1     0.000000     0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "2     0.000000     0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "3     0.000000     0.337638  0.000000  ...  0.000000  0.565934  0.337638   \n",
       "4     0.388496     0.000000  0.000000  ...  0.492675  0.000000  0.000000   \n",
       "5     0.000000     0.000000  0.321076  ...  0.407176  0.000000  0.000000   \n",
       "6     0.000000     0.000000  0.000000  ...  0.263081  0.000000  0.000000   \n",
       "7     0.000000     0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "\n",
       "         so  somewhat      sort     still     there      well       yet  \n",
       "0  0.000000  0.000000  0.000000  0.241098  0.000000  0.000000  0.000000  \n",
       "1  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "2  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "3  0.000000  0.337638  0.000000  0.379013  0.000000  0.282967  0.000000  \n",
       "4  0.388496  0.000000  0.000000  0.218051  0.000000  0.325590  0.388496  \n",
       "5  0.000000  0.000000  0.000000  0.000000  0.321076  0.000000  0.000000  \n",
       "6  0.000000  0.000000  0.414902  0.232872  0.000000  0.000000  0.000000  \n",
       "7  0.000000  0.000000  0.000000  0.270197  0.000000  0.000000  0.000000  \n",
       "\n",
       "[8 rows x 32 columns]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tf_advs_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "#11.5.2.1 Final shape\n",
    "cols = df_tf_advs_idf.columns\n",
    "bt = df_tf_advs_idf.apply(lambda x: x > 0)\n",
    "final_shape_advs = [bt.apply(lambda x: list(cols[x.values]), axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0    [clearly, definitely, however, real, really, r...\n",
      "1                                               [kind]\n",
      "2                                [mentally, overnight]\n",
      "3    [almost, essentially, right, slightly, somewha...\n",
      "4    [absolutely, effectively, really, so, still, w...\n",
      "5    [again, best, even, kind, privately, quickly, ...\n",
      "6      [further, kind, knowingly, really, sort, still]\n",
      "7            [alone, fortunate, nearby, pretty, still]\n",
      "dtype: object]\n"
     ]
    }
   ],
   "source": [
    "print(final_shape_advs) #might be better as dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
